\chapter{Computational methodology} \label{sec:CompMethodology}


Subsequently, the following section X TODO introduces the computational methodology that allows the resolution of the discretized equations. First, the globalized Newton method that allows the resolution of the nonlinear system of equations is presented. In addition, comments on the termination criteria are given.


The presented solver is embedded in the \textit{BoSSS} (Bounded Support Spectral Solver) code, which is under development at the chair of fluid dynamics of the Technical University of Darmstadt, and is available under \href{https://github.com/FDYdarmstadt/BoSSS}{https://github.com/FDYdarmstadt/BoSSS}. \BoSSS is a general framework for the discretization of conservation laws using the DG method and uses a modal DG approach with orthonormal Legendre polynomials as basis functions. The \BoSSS code features a variety of applications in the context of computational fluid dynamics, such as a solver for multiphase flows with a sharp interface approach, \textcite{kummerExtendedDiscontinuousGalerkin2017} an incompressible Immersed Boundary Method solver for particle laden flows,\textcite{krauseIncompressibleImmersedBoundary2017} a solver for viscoelastic fluid flows,\textcite{kikkerFullyCoupledHighorder} and a solver for compressible flows, \textcite{geisenhoferDiscontinuousGalerkinImmersed2019} among others.

The variational problem for the finite chemistry rate case described by \crefrange{DiscretizedConti}{DiscretizedMassFractions} and for the flame sheet problem, \crefrange{DiscretizedConti2}{DiscretizedEnergy2}, are linearised and solved using a Newton method with a Dogleg-type globalization \textcite{pawlowskiGlobalizationTechniquesNewton2006,pawlowskiInexactNewtonDogleg2008}. The \textit{BoSSS} framework provides an efficient algorithm for the evaluation of the Jacobian based on perturbations of the forms presented in the last section.
The relatively large linear system of equations for each Newton iteration is solved by means of the in \BoSSS integrated orthonormalization multigrid algorithm \textcite{kummerBoSSSPackageMultigrid2021}, which at the lowest multigrid levels makes use of the sparse direct solver PARDISO, originally developed by Schenk et al. \textcite{schenkEfficientSparseLU2000,schenkTwolevelDynamicScheduling2002,schenkSolvingUnsymmetricSparse2004a},
from the ``Intel(R) Parallel Studio XE 2018 Update 3 Cluster Edition for Windows'' library collection to solve the linear system.

\textit{BoSSS} also features a method for solving highly nonlinear problems with a homotopy strategy.
Further details on the used Newton method solver, the homotopy strategy and its implementation are given in the next sections, which are adapted from \textcite{kikkerFullyCoupledHighorder}  and are included for the sake of completeness. For information on the mentioned orthonormalization multigrid algorithm we refer the interested reader to the work of \textcite{kummerBoSSSPackageMultigrid2021}.

From this point on, the solver presented in this work will be called XNSEC (e\textbf{X}tended \textbf{N}avier-\textbf{S}tokes for \textbf{C}ombustion). The term \textit{extended} refers to the framework on which the solver is built, which focuses on applications for multi-phase flows using a sharp interface approach using a level-set method. This point will be briefly discussed in the section %TODO future work chapter

\subsection{Calculation of the Jacobian matrix}
We start by introducing a few new elements to be able to describe the implemented Newton method.
The discussion for the variational problem using the mixture fraction variable (\crefrange{DiscretizedConti2}{DiscretizedEnergy2}) is completely analogous, and will not be mentioned in this discussions.
%The variational problem defined by \cref*{DiscretizedConti,DiscretizedMomentum,DiscretizedEnergy,DiscretizedMassFractions} can be cast into a more compact notation. By subtracting all terms from the right-hand sides from the terms of the left-hand sides of \crefrange{DiscretizedConti}{DiscretizedMassFractions} the problem can be written as:
Find $\myvector{U}_h \in \mathbb{V}_\myvector{k}$
\begin{equation}
	\mathcal{N}(\myvector{U}_h,\myvector{V}_h) = 0 \quad \forall \ \myvector{V}_h \in \mathbb{V}_\myvector{k} ,
	\label{eq:CompactVariational}
\end{equation}
for
$\myvector{U}_h = (p_h,\vec{u}_h, T_h, \MFVecPrima_h)$ and
$\myvector{V}_h = (q_h,\vec{v}_h, r_h, \mathbf{s}_h)$
. We assume a basis
$\underline{\gvec{\Phi}} = ( \gvec{\Phi}_1, \ldots , \gvec{\Phi}_L )$ of $\mathbb{V}_\myvector{k}$,
written as a row vector, with $L \coloneqq \textrm{dim}(\mathbb{V}_\myvector{k})$.
Then $\myvector{U}_h$ can be represented as
$ \myvector{U}_h =  \underline{ \gvec{\Phi} } \cdot \myvector{U} $.
The nonlinear problem (\ref{eq:CompactVariational}) can then be written as
\begin{equation}
	\mathcal{A}(\myvector{U}) = 0 ,
	\label{Eq:nonLinSystem}
\end{equation}
with the nonlinear function
$\mathbb{R}^L \ni \myvector{U} \mapsto \mathcal{A}(\myvector{U}) \in \mathbb{R}^L$.
The $i$-th component of $ \mathcal{A}(\myvector{U})$, can be defined by $\mathcal{N}(-,-)$ through the relation
$[\mathcal{A}(\myvector{U})]_i = \mathcal{N}( \underline{ \gvec{\Phi} } \cdot \myvector{U} , \gvec{\Phi}_i)$.

The formulation of the Newton method requires the Jacobian matrix   $\partial \mathcal{A}$ of $\mathcal{A}$, defined as
\begin{equation}
	\partial \mathcal{A}_{ij}(\myvector{U}) \coloneqq \frac{\partial \mathcal{A}_i}{\partial U_j}(\myvector{U}).
	\label{Eq:Jacobian}
\end{equation}
Its computation is quite straightforward, but lengthy. The \BoSSS code is capable of evaluating the
Jacobian matrix automatically from the expressions given in Section \ref{ssec:SpatDiscretization}.
We note that one could write $\mathcal{A}(\myvector{U})$ as
\begin{equation}
	[\mathcal{A}(\myvector{U})]_i = \mathcal{N}( \myvector{U}_h , \gvec{\Phi}_i) =
	\int_{\Omega_h}
	N_1 (\vec{x}, \myvector{U}_h , \nabla \myvector{U}_h  ) \cdot \gvec{\Phi}_i
	+ N_2 (\vec{x}, \myvector{U}_h , \nabla \myvector{U}_h  ) \cdot \nabla \gvec{\Phi}_i
	\textrm{dV}
	+
	\oint_{\Gamma} \ldots \mathrm{dS}.
	\label{eq:NonlinearGestalt}
\end{equation}
The edge integral, which is left out in \cref{eq:NonlinearGestalt},
can be written in analogous fashion as the volume integral, i.e. as a sum over
four nonlinear functions, multiplied by
$ \gvec{\Phi}_i^{+}$,  $\gvec{\Phi}_i^{-}$, $ \nabla \gvec{\Phi}_i^{+}$ and  $ \nabla \gvec{\Phi}_i^{-}$,
respectively.
These functions themselves may depend on
$\vec{x}$, $\myvector{U}_h^{+}$,  $\myvector{U}_h^{-}$, $\nabla \myvector{U}_h^{+}$ and  $\nabla \myvector{U}_h^{-}$.
For sake of compactness, this part is skipped.
Realizing that $\frac{\partial \myvector{U}_h}{\partial \myvector{U}_j}  = \gvec{\Phi}_j$ and by application of the
chain rule, one derives
\begin{equation}
	\partial \mathcal{A}_{ij}(\myvector{U}) =
	\int_{\Omega_h}
	( \partial_{ \myvector{U}_h}       N_1 (\vec{x}, \myvector{U}_h , \nabla \myvector{U}_h  ) \gvec{\Phi}_j
	+   \partial_{\nabla \myvector{U}_h} N_1 (\vec{x}, \myvector{U}_h , \nabla \myvector{U}_h  ) \nabla \gvec{\Phi}_j ) \cdot \gvec{\Phi}_i
	+ \ldots
	\textrm{dV}
	+
	\oint_{\Gamma} \ldots \mathrm{dS} .
	\label{eq:JacobiGestalt}
\end{equation}
All skipped terms in \cref{eq:JacobiGestalt} can be derived in an analogous fashion as the contributions for $N_1$.
In the \BoSSS code, derivatives $ \partial_{ \myvector{U}_h} N_1 ( \ldots )$ and  $ \partial_{ \nabla \myvector{U}_h} N_1 ( \ldots )$
are approximated by a finite difference, using a perturbation by $\sqrt{\mathtt{eps}}$ in the respective argument,
where  $\mathtt{eps} = 2.22044604925031 \cdot 10^{-16}$ is the floating point accuracy for double precision.

The notation introduced here allows us to describe the Newton-Dogleg method used in this work. We note however that this globalization strategy is still not sufficient to ensure convergence for some of the test cases presented, namely for high Rayleigh numbers for the differentially heated cavity problem. For those cases we use a homotopy strategy, where we start with a low homotopy-parameter, a parameter for which the solution of the problem is not hard to find, which is gradually and carefully increased until convergence for the desired value of the homotopy-parameter is reached (cf. \cref{sec:HomotopyMethod}).


\subsection{Dogleg Method} \label{sec:newton}
We consider a linearization of \cref{Eq:nonLinSystem} around $\myvector{U}_n$,
\begin{equation}
	\mathcal{A}(\myvector{U}_{n}) +
	\partial \mathcal{A} (\myvector{U}_n) \underbrace{ ( \myvector{U}_{n+1} -  \myvector{U}_{n} ) }_{=: \myvector{s}'_n }
	= 0.
	\label{eq:LinearizedSys}
\end{equation}
By repeatedly solving this system one obtains
a standard Newton scheme for \cref{Eq:nonLinSystem},
yielding a sequence of approximate solutions
$
\myvector{U}_0, \myvector{U}_1, \myvector{U}_2, \ldots
$
obtained from an initial guess $\myvector{U}_0$ through the iteration scheme
$
\myvector{U}_{n+1} = \myvector{U}_n + \myvector{s}'_n.
$
In the classical un-damped Newton method, the correction step $\myvector{s}'_n$ is set to be the whole Newton-step,
i.e  $\myvector{s}'_n = \myvector{s}_n$ with
\begin{equation}
	\myvector{s}_n  \coloneqq - \partial \mathcal{A}(\myvector{U}_n)^{-1}\mathcal{A}(\myvector{U}_n),
	\label{eq:NewtonStep}
\end{equation}
which is computed using a direct solver.
Unfortunately, convergence of the Newton method for any starting value $\myvector{U}_0$ is not guaranteed.
In order to increase robustness when the distance between $\myvector{U}_0$ and the exact solution $\myvector{U}$ is large,
we employ a globalization approach, presented by Pawlowski et al. \textcite{pawlowskiGlobalizationTechniquesNewton2006,pawlowskiInexactNewtonDogleg2008},
known as the Dogleg-method, or Newton-Dogleg method.
Here, we intend to give only the central ideas of method and refer to the original works for further details.
Obviously, the exact solution of \cref{Eq:nonLinSystem} is also a minimum of the functional
\begin{equation}
	f(\myvector{U}) \coloneqq \frac{1}{2}  \left\| \mathcal{A}(\myvector{U})  \right\|^2_2 .
\end{equation}
One observes that $\nabla f(\myvector{U}) = \partial \mathcal{A}(\myvector{U})^T \mathcal{A}(\myvector{U})$.
For $\myvector{U}_n$, the approximate Cauchy point with respect to the 2-norm,
is defined as the minimizer $\myvector{g}_n$ of
$ \left\|  \mathcal{A}(\myvector{U}_{n}) + \partial \mathcal{A} (\myvector{U}_n) \myvector{g}_n \right\|_2  $
in the direction of steepest decent, i.e. $\myvector{g}_n = \lambda \nabla f(\myvector{U}_n)$, $\lambda \in \mathbb{R}$.
Substituting $\myvector{w} \coloneqq - \partial \mathcal{A}(\myvector{U}_n) \nabla f(\myvector{U}_n)$, $\myvector{g}_n$ is given by
\begin{equation}
	\myvector{g}_n = \frac{\mathcal{A}(\myvector{U}_n) \cdot \myvector{w}}{\myvector{w} \cdot \myvector{w}} \nabla f(\myvector{U}_n) .
	\label{eq:CauchyPoint}
\end{equation}

For the Newton-Dogleg method, the correction step  $\myvector{s}'_n$
is chosen along the so-called Dogleg curve, which is the piece-wise linear
curve from the origin to $\myvector{g}_n$ and further to $ \myvector{s}_n$.
The selection of $\myvector{s}'_n$ on this curve is determined by the trust-region diameter $\delta > 0$:
\begin{itemize}
	\item
	If $\|  \myvector{s}_n \|_2 \leq \delta$, $ \myvector{s}'_n =  \myvector{s}_n$.
	
	\item
	If  $\|  \myvector{g}_n \|_2 \leq \delta$ and $\|  \myvector{s}_n \|_2 > \delta$,
	$\myvector{s}'_n$ is chosen on the linear interpolation from $\myvector{g}_n$ to $\myvector{s}_n$
	so that  $\|  \myvector{s}'_n \|_2 = \delta$:
	For the ansatz
	$\myvector{s}'_n = \tau \myvector{s}_n + (1-\tau) \myvector{g}_n$,
	the interpolation factor $\tau$ is given as
	$ \tau = (a^2 - c + \sqrt{(a^2 + b^2 - 2 c) \delta^2 - a^2 b^2 + c^2}) / (a^2 + b^2 - 2 c) $
	with $a = \| \myvector{g}_n \|_2$,  $b = \| \myvector{s}_n \|_2$ and $c = \myvector{g}_n \cdot \myvector{s}_n $.
	
	\item
	If  $\|  \myvector{g}_n \|_2 > \delta$,
	$  \myvector{g}_n = (\delta / \|  \myvector{g}_n \|_2) \myvector{g}_n$.
\end{itemize}
The choice and adaptation of the trust region diameter $\delta$
throughout the Newton-Dogleg procedure follows
a sophisticated heuristic,
mainly based on comparing the actual residual reduction
$\textrm{ared}_n \coloneqq \| \mathcal{A} (\myvector{U}_n) \|_2 - \| \mathcal{A} (\myvector{U}_n  + \myvector{s}'_{n} ) \|_2$
with the predicted residual reduction
$\textrm{pred}_n \coloneqq \| \mathcal{A} (\myvector{U}_n) \|_2 - \| \mathcal{A} (\myvector{U}_n )   + \partial \mathcal{A} (\myvector{U}_n ) \myvector{s}'_{n} \|_2$; For the direct solver used in this work $\textrm{pred}_n$ simplifies to
$\textrm{pred}_n \coloneqq \| \mathcal{A} (\myvector{U}_n) \|_2$.
We replicate the algorithm here, for the sake of completeness:
\begin{itemize}
	\item[(1)]
	Set $n=0$, $\delta_n = \min(10^{10}, \max(2 \cdot 10^{-6}, \| \myvector{s}_0 \|_2 ))$.
	
	\item[(2)]
	Compute the Newton step $\myvector{s}_n$ and the Cauchy point  $\myvector{g}_n$ and
	find $\myvector{s}'_n$ on the Dogleg curve with respect to the recent $\delta_n$.
	
	\item[(3)]
	While $\textrm{ared}_n \leq \textrm{pred}_n$ do:
	Update trust region diameter $\delta_n \leftarrow 0.5 \ \delta_n$
	and re-compute $\myvector{s}'_n$.
	If $\delta_n < 10^{-6}$ terminate abnormally and mark the computation as failed.
	
	\item[(4)]
	If the convergence criterion (see below) is fulfilled, terminate and mark the computation as success.
	
	\item[(5)]
	Perform a final update of the trust region: Set
	\[
	\delta_{n+1} = \left\{ \begin{array}{ll}
		\max( 10^{-6}, \| \myvector{s}_n \|_2 ) & \text{if } \textrm{ared}_n / \textrm{pred}_n < 0.1 \text{ and } \| \myvector{s}_n \|_2 \delta_n \\
		\max( 10^{-6}, 0.25 \cdot \delta_n )    & \text{else, if } \textrm{ared}_n / \textrm{pred}_n < 0.1                                        \\
		\min( 10^{10}, 4 \cdot \delta_n )       & \text{else, if } \textrm{ared}_n / \textrm{pred}_n > 0.75                                       \\
		\delta_{n}                              & \text{otherwise}                                                                                \\
	\end{array} \right.
	\]
	Set $\myvector{U}_{n+1} = \myvector{U}_{n} + \myvector{s}'_{n} $, update $n \leftarrow n + 1$ and return to step (2).
	
\end{itemize}
All constants used in the algorithm above have been taken from the work of Pawlowski et al. \textcite{pawlowskiGlobalizationTechniquesNewton2006}
For a detailed description of the underlying ideas we also refer to these works,
which in turn are based on algorithms from Dennis and Schnabel's textbook. \textcite{dennisNumericalMethodsUnconstrained1996}


\subsection{Termination criterion}
\label{ssec:TerminationCriterion}
A simple approach to determine that a Newton-Dogleg loop can be terminated
is to check whether the residual norm has fallen below a certain threshold, i.e.
$ \| \mathcal{A}(U_n) \| \leq \textrm{tol}  $.
A universal choice for the tolerance is indeed difficult,
especially for investigations of convergence properties (cf.  \cref{ssec:ConvStudyHeatedCavity} and \cref{ss:UDF}).
If it is chosen too low, the algorithm may never terminate, because of dominating numerical round-off errors. On the other hand, if it is chosen too high, the error of the premature termination may dominate  the error of
the spatial discretization and one cannot take the full advantage of the high-order method.
Therefore the goal is to continue the Newton-Dogleg method until
the lowest possible limit dictated by floating point accuracy is reached.
To identify the limit in a robust way, we first define the residual-norm skyline as
\begin{equation}
	\textrm{sr}_n \coloneqq \min_{j \leq n} \| \mathcal{A}(\myvector{U}_j) \|
\end{equation}
and, for $n \geq 2$, the averaged reduction factor
\begin{equation}
	\textrm{arf}_n \coloneqq \frac{1}{2} \left(
	\frac{ \textrm{sr}_{n-2} }{  \max \{ \textrm{sr}_{n-1}, 10^{-100} \} }
	+  \frac{ \textrm{sr}_{n-1} }{  \max \{ \textrm{sr}_{n},   10^{-100} \} }
	\right) .
\end{equation}
The Newton-Dogleg method is terminated if
\begin{equation}
	n \geq 2 \text{ and }
	\textrm{sr}_n \leq 10^{-5} + 10^{-5} \| \myvector{U}_n \|_2 \text{ and }
	\textrm{arf}_n < 1.5 .
\end{equation}
For the computations in this work, this choice guarantees that the
nonlinear system is solved as accurately as possible. It secures that the numerical error is dominated by the error of the spatial or temporal discretization
and not by the termination criterion of the Newton-Dogleg method.
The skyline approach ensures robustness against oscillations close to the lower limit.
\subsection{Solver safeguard}
%TODO Bounding of values using the solver safeguard option. Manual modification of the proposed solution given by the solver in order to avoid unphysical solutions
/// 'safeguard' for solvers to avoid unphysical solutions during the solution procedure;
/// An example would be to avoid e.g. negative denities, which might even cause NaNs,
/// during the solver run for implicit, nonlinear equations.
from \textcite{nicoudConservativeHighOrderFiniteDifference2000}
 For these scales the dispersion phenomenon is important
and may generate negative values of temperature if a non-positive scheme
is used. The clipping to values greater than the cold wall temperature is
justified since there is no physical mechanism which can decrease the temperature below this point. It emulates a TVD or ENO scheme which should
be used for temperature. Since such techniques are well known but beyond
the scope of this study, we preferred to use the simple clipping approach
T > T1 when necessary. 
\subsection{Homotopy method}
\label{sec:HomotopyMethod}
Although the Newton-Dogleg method works well for a variety of cases,
we experienced convergence problems for some of the test cases presented in next section.
In particular, for the differentially heated cavity test case, the method was not successful on finding a convergent solution for a Rayleigh number $Ra \geq 10^5$  within 60 Newton iterations.
In such cases we used a homotopy strategy, which is loosely based on
ideas from the textbook of Deuflhard,\textcite{deuflhardNewtonMethodsNonlinear2011} Chapter 5.
\tikzexternaldisable
\begin{figure}[t]
	\centering
	\pgfplotsset{
		compat=1.3,
		tick align = outside,
		yticklabel style={/pgf/number format/fixed},
	}
	\begin{tikzpicture}
		\begin{axis}[
			set layers=standard,
			width=0.75\linewidth,
			height=6cm,
			axis y line*=left,
			ymode = log,
			xlabel=Iteration number,
			xmin = 0,
			ylabel=Residual and trust region $\delta$,
			xtick = {0,2,...,36},
			ytickten = {-16,-12,...,12}
			]
			\addplot[ blue, mark =square*, mark size = 2pt] file{data/ConvergenceStory_HeatedCavity_withHomotopy/delta3.txt};\label{plot_one}
			\addplot[ orange, mark =o, mark size = 2.5pt] file{data/ConvergenceStory_HeatedCavity_withHomotopy/residuals3.txt};\label{plot_two}
		\end{axis}
		\begin{axis}[
			axis y line=right,
			width=0.75\linewidth,
			height=6cm,
			axis x line=none,
			ylabel= Homotopy parameter hp,
			xmin = 0,
			ymin = 50,
			ymax = 1100,
			legend style={at={(0.1,0.99)},anchor=north west,},
			xtick={0,5,...,35},
			ytick ={100,200,...,1000},
			]
			\addlegendimage{/pgfplots/refstyle=plot_two}\addlegendentry{$	\| \mathcal{A}_{\mathrm{hp}^*}(\myvector{U}_{n}) \|_2 $}
			\addlegendimage{/pgfplots/refstyle=plot_one}\addlegendentry{$\delta$}
			\addplot[red , mark =x, mark size = 2.5pt] file{data/ConvergenceStory_HeatedCavity_withHomotopy/reynolds3.txt};
			\addlegendentry{$\mathrm{hp}$}
		\end{axis}
	\end{tikzpicture}
	\caption{Behaviour of the homotopy method for the differentially heated cavity test case. The homotopy parameter $\mathrm{hp}$ in this case is the Reynolds number. }
	\label{fig:Homotopyevolution}
\end{figure}

\tikzexternalenable
We start by identifying a parameter that makes the solution of the nonlinear problem difficult to solve. In the following we will refer to this variable as the homotopy parameter.
The main idea of the homotopy strategy consists of solving a series of simpler problems, starting with a parameter where the problem is easy to solve, and carefully increasing it until the desired value is reached.  Let $\mathrm{Hp}$ denote the value of the homotopy parameter for which a solution is being sought. Let
\begin{equation}
	\mathcal{A}_{\mathrm{hp}^*}(\myvector{U}) = 0
	\label{eq:NonlinearAt-hp}
\end{equation}
be the discretized system for a certain intermediate homotopy-parameter
$\mathrm{hp}^*$, between 0 and the `target'  homotopy-parameter $\mathrm{Hp}$, i.e. $0 \leq \mathrm{hp}^* \leq \mathrm{Hp}$.
Furthermore, let $\myvector{U}_{\mathrm{hp},\epsilon} $ be an approximate solution
to the problem (\ref{eq:NonlinearAt-hp}) with $ \mathrm{hp}^* =  \mathrm{hp}$,
up to a tolerance $\epsilon$, i.e.
\begin{equation}
	\left\| \mathcal{A}_{\mathrm{hp}}( \myvector{U}_{\mathrm{hp},\epsilon} ) \right\|_2 \leq \epsilon .
\end{equation}
For the sake of clarity when discussing the algorithm which follows below, we distinguish between the
intermediate homotopy-parameter  $\mathrm{hp}$ for which we assume to already have found an acceptable solution
and the next homotopy-parameter $\mathrm{hp}^*$ that we are currently trying to find a solution for.
For any $\textrm{hp}^* > \textrm{hp}$
we set
$
\epsilon
= 10^{-5}
\left\| \mathcal{A}_{\mathrm{hp*}}( \myvector{U}_{\mathrm{hp},\epsilon} ) \right\|_2
$,
i.e. we aim for a residual norm reduction of at least five orders of magnitude with respect to the initial residual norm.
If  $\textrm{hp}^* = \textrm{Hp}$, the termination criterion presented in section \ref{ssec:TerminationCriterion} is applied.
An approximate solution for the target homotopy-parameter is found by the following recipe:
\begin{itemize}
	\item[(1)]
	Set $\mathrm{hp} = 0$, i.e. start by obtaining an (approximate) solution $\myvector{U}_{0,\epsilon}$.
	
	\item[(2)]
	Search for a an increased homotopy-parameter $\mathrm{hp}^*$:
	Find the minimal $i \geq 0$ so that for
	$
	\mathrm{hp}^* = \frac{1}{2^i}(\mathrm{Hp} - \mathrm{hp}) + \mathrm{hp}
	$
	one has
	$
	\left\| \mathcal{A}_{\mathrm{hp}^*}(\myvector{U}_{\mathrm{hp},\epsilon}) \right\|_2
	\leq
	\delta_{\textrm{max}} \left\| \mathcal{A}_{\mathrm{hp}}(\myvector{U}_{\mathrm{hp},\epsilon}) \right\|_2
	$
	Here, $\delta_{\textrm{max}}$ is the maximal allowed increase of the residual for an increased
	homotopy-parameter $\mathrm{hp}^*$;  $\delta_{\textrm{max}}$ is adapted in the following steps,
	as an initial guess we use $ \delta_{\textrm{max}} = 10^6$.
	
	\item[(3)]
	Use the Newton-Dogleg method to compute an approximate solution to the problem (\ref{eq:NonlinearAt-hp}),
	for the homotopy-parameter $\mathrm{hp}^*$,
	using the solution $\myvector{U}_{\mathrm{hp},\epsilon}$ as an initial guess.
	
	\begin{itemize}
		\item
		If the Newton-Dogleg method did not converge successfully within ten steps,
		the homotopy-parameter increase from $\mathrm{hp}$ to $\mathrm{hp}^*$ was probably too large.
		Set $\delta_{\textrm{max}} \leftarrow 0.2\delta_{\textrm{max}}$ and go to step (2).
		
		\item
		If the Newton-Dogleg method reached its convergence criterion and
		if the target homotopy-parameter is reached, i.e. $\mathrm{hp}^* = \mathrm{Hp}$,
		the algorithm has successfully found an approximate solution
		for $ \mathcal{A}_{\mathrm{Hp}}(\myvector{U}) = 0$ and can terminate.
		
		\item
		Otherwise, if the Newton-Dogleg method converged successfully, but is below the target homotopy-parameter:
		Accept the solution and set $\mathrm{hp} \leftarrow \mathrm{hp}^*$.
		If the Newton-Dogleg method took less than three iterations to reach the convergence criterion,
		set  $\delta_{\textrm{max}} \leftarrow 8\delta_{\textrm{max}}$.
		Return to step (2).
	\end{itemize}
	
\end{itemize}
An exemplary run of the method is shown in \cref{fig:Homotopyevolution}. The homotopy parameter $\mathrm{hp}$ in this particular case is the Reynolds number. The homotopy-parameter hp was increased for iterations 10, 18, 22 and 24, causing an increase of the residuals $\| \mathcal{A}_{\mathrm{hp}^*}(\myvector{U}_{n}) \|_2 $, leading to a convergent solution after 34 Newton iterations. The presented algorithm offers a robust method for finding steady-state solutions of highly nonlinear systems.

\subsection{Initialization of combustion applications}\label{ssec:MethodCombustion}
Cases involving combustion are initialized with the solution of the flame sheet problem, \cref{DiscretizedConti2,DiscretizedMomentum2,DiscretizedEnergy2}. This idea has been already employed in various works. \textcite{smookeNumericalSolutionTwoDimensional1986,smookeNumericalModelingAxisymmetric1992} The reason for the use of this pre-step is twofold:
\begin{itemize}
	\item Solving \cref{eq:LowMachConti,eq:LowMachMomentum,eq:LowMachEnergy,eq:LowMachMassBalance} using a Newton-type method requires adequate starting estimates in order to converge. Using the flame sheet solution as initial estimate improves the convergence properties of the method.
	\item The system of  \cref*{eq:LowMachConti,eq:LowMachMomentum,eq:LowMachEnergy,eq:LowMachMassBalance} possesses multiple solutions. One is the pure mixing (frozen) solution, where no chemical reaction has taken place, and the other one is the ignited solution, where the flame is present. Using the flame sheet solution as initial estimate ensures that the path taken by Newton's algorithm will tend towards the ignited solution.
\end{itemize}

It should be noted regarding the solution of the flame sheet problem (cf. \cref{sec:FlameSheet}) that the sharp change in the primitive variables around $z = z_{st}$  is problematic in certain scenarios. In particular, the non-smoothness of the derived variables could lead to Gibbs phenomenon-type problems. This inconvenient can be remedied by using a regularized form of the equations. The smoothing function $\mathcal{H}$ is defined as
\begin{equation}\label{eq:regularization_MF}
	\mathcal{H}(z) \approx \frac{1}{2}(1+\tanh(\frac{z - z_{st} }{\sigma} )),
\end{equation}

\begin{figure}[t]
	\centering
	\inputtikz{SmoothingFunc}
	\caption{Smoothing function for different $\sigma$ and $z_{\text{st}}$}\label{fig:SmoothingFunc}
\end{figure}

This function is useful for creating a smooth transition between two functions, since it returns values close to 0 for $z \ll z_{st}$ and values close to 1 for $z \gg z_{st}$. The sharpness of the transition at the point $z = z_{st}$ is dictated by the parameter $\sigma$. In \cref{fig:smoothings} the effect of the smoothing factor $\sigma$ on calculations of a flame in a counter-flow configuration are shown. It can be clearly observed how for increasing $\sigma$ the solution becomes smoother. Using \cref{eq:regularization_MF} the temperature and mass fraction fields can be written as
\begin{subequations}
	\begin{align}
		T(z)   & = z T_F^0 + (1-z)T_O^0 + \frac{Q Y_F^0}{c_p} z_{st}\frac{1- z}{1-z_{st}}\mathcal{H}(z) +  \frac{Q Y_F^0}{c_p}z\left(1-\mathcal{H}(z)\right),  \label{eq:BS-TR} \\[1ex]
		Y_F(z) & = Y_F^0\frac{z - z_{st}}{1-z_{st}} \mathcal{H}(z), \label{eq:BS-YFR}                                                                                           \\[1ex]
		Y_O(z) & = Y_O^0 \frac{z_{st}-z}{z_{st}} (1-\mathcal{H}(z)), \label{eq:BS-YOR}                                                                                          \\[1ex]
		Y_P(z) & =  Y_O^0\frac{M_P\nu_P}{M_O\nu_O}(1-z)\mathcal{H}(z) +	Y_F^0\frac{M_P\nu_P}{M_F\nu_F}z (1-\mathcal{H}(z)), \label{eq:BS-YPR}                                   \\[1ex]
		Y_N(z) & = (1-Y_F^0)z + (1-Y_O^0)(1-z). \label{eq:BS-YNR}
	\end{align}
\end{subequations}
The use of this regularized form of the equations results in practice on a spreading of the flame front, which eases the numerical calculation \parencite{braackAdaptiveFiniteElement1997}.

One question one could certainly ask is under what flame conditions the infinite rection-rate solution (also called flame-sheet solution in the following) effectively is a good initial estimate for Newton's algorithm.  Obviously for systems that respect the assumptions done for the flame-sheet the obtained solution will be very close to the finite-rate solution (see \cref{fig:MixtureFraction_finiteRateComparison}). 

The assumption of an infinitely fast chemical reaction implies that the time scales associated with the chemical reaction are infinitely smaller than the flow scales, or in other words, $\text{Da} \to \infty$. For this reason, the flame sheet solution is expected to give a similar solution for cases close to equilibrium (where the Damk√∂hler number is large). On the other hand, in cases that are far from equilibrium, as, for example, in the case of a flame in conditions close to extinction, it is expected that the flame sheet solution will depart considerably from the solution with a finite reaction rate.



It should be noted that within the derivation of the equations for the flame sheet it is only assumed that the heat capacity is the same for all components ($c_{kp} = c_p$), but it is still possible to consider a dependence on temperature. However, this introduces a difficulty, since the evaluation of the temperature with \cref{eq:BS-YF} requires $c_p$, which according to \cref{eq:nondim_cpmixture}, depends in turn on the temperature. Solving the system of equations required to obtain $c_p$ and $T$ is very expensive, since it would require solving it every time the temperature must be evaluated -in particular for the evaluation of the density $\rho$ and transport parameters $\mu$ and $\rho D$-.  This problem can be solved by simply assuming a constant representative value of $c_p$. 

The problem that now arises is the selection of a suitable $c_p$. In the work by \textcite{xuApplicationPrimitiveVariable1993} it is suggested to estimate it simply on the basis of experimental measurements, or also by selecting some representative value, such as $c_p$ evaluated at the adiabatic temperature and stoichiometric conditions. In particular, in this work the value $\hat c_p = \SI{1.3}{\kilo \joule \per \kilo \gram \per \kelvin}$ was adequate for all calculations. This constant value of the heat capacity proved to be an adequate estimate for finite-rate simulations, even for cases with a nonconstant heat capacity.

In a similar fashion, the assumption of unity Lewis number in the flame sheet system delivers a solution that slightly deviates from the solution of the finite chemistry rate problem with non-unity Lewis numbers. Nevertheless, this small deviation does not preclude the use of the flame sheet solution as an adequate initial estimate for Newton's method. 






\begin{figure}[t!]
	\begin{center}
		\def\svgwidth{0.5\textwidth}
		\import{./plots/}{SShapedResponse.pdf_tex}
		\caption{S-shaped bifurcation curve of a combustion process.}
		\label{fig:Sshaped}%
	\end{center}%
\end{figure}%


\begin{figure}
	\centering
	\inputtikz{SmoothingPicture}
	\caption{Temperature profile calculated in the center-line of a counter-flow flame configuration for different smoothing parameters $\sigma$.}
	\label{fig:smoothings}
\end{figure}

\subsubsection{Adaptive Mesh Refinement}\label{ssec:MeshRefinement}
The area where the chemical reaction takes place is usually a thin region whose thickness is defined by the availability of reactants.
\begin{figure}
	\centering
	\pgfplotsset{width=0.50\textwidth, compat=1.3}
	\inputtikz{MeshRefinementCoflow1}
		\inputtikz{MeshRefinementCoflow2}
			\inputtikz{MeshRefinementCoflow3}
	\caption{Adaptive mesh refinement around the stoichiometric surface in a coflow flame configuration.}
\end{figure}