\chapter{Computational methodology} \label{ch:CompMethodology}
This chapter addresses the computational methods developed for solving the system of governing equations presented in the preceding chapters. The chapter starts in \cref{sec:StateOFArt} with a concise literature review concerning the methods employed in this study. Subsequently, in \cref{sec:XNSECSolver} a comprehensive description of the methods employed by the XNSEC solver is provided. Then in \cref{sec:SolNonLinProblem} the implementation and uses of both nonlinear and linear solvers are presented. Finally, in \cref{sec:ConvSupportStrat} various convergence-enhancing strategies employed for solving highly nonlinear problems are presented, such as the use of a homotopy strategy for highly non-linear problems, or the use of solver safeguards to avoid nonphysical results. 

\section{State of the art of DG methods for diffusion flame simulations} \label{sec:StateOFArt}
There are numerous works in which the \Gls{DG} method has been used in the context of \Gls{CFD}. Extensive attention has been paid to algorithms for solving incompressible flows, such as in the works from  \textcite{shahbaziHighorderDiscontinuousGalerkin2007,kummerBoSSSDiscontinuousGalerkin2012,kleinSIMPLEBasedDiscontinuous2013}. However, not much attention has been put into extending the methods to be able to handle variable density flows. In the work by \textcite{kleinHighorderDiscontinuousGalerkin2016} an extension is presented of the method for solving  the low-Mach equations in a DG Framework making use of a SIMPLE type scheme. As mentioned in the introduction, while the strategy proved useful for simulating various flow problems, for many of these problems the computation time proved to be prohibitive. Similarly, in the work of  \textcite{henninkPressurebasedSolverLowMach2021} a pressure-based solver for low-Mach flows is presented. They solve the system of equations in a segregated way, and solve for the mass flux instead of the velocity as the primitive variable. They employ an equal order formulation for the coupling of pressure and velocity; however, this approach results in significant computational expenses, especially when dealing with high Reynolds numbers. They solve the enthalpy form of the energy equations, which is shown to be a source of instabilities if special care is not taken. 

In the context of \Gls{CFD}, the coupling between velocity fields and pressure within the Navier--Stokes equations has been a topic of considerable research and interest. It's a well known fact that in the case of incompressible flows, the absence of a time derivative in the continuity equation poses challenges for advancing the system in time using conventional methods. Traditionally this type of coupling has been solved by means of segregated methods, such as projection methods \parencite{chorinNumericalSolutionNavierStokes1967}, or by using methods such as SIMPLE \parencite{patankarNumericalHeatTransfer1980}. Such algorithms offer the benefit of low memory usage and have undergone extensive investigation and application especially within the context of approaches like \gls{FVM}. However, as mentioned earlier, the use of segregated methods doesn't guarantee conservativity of the relevant physical quantities, and special care has to be taken in that regard. Since for low-Mach number flows the flow field is coupled to the temperature and therefore to the density, it is not possible to solve the equations in a completely conservative way in a non-iterative segregated framework. 

The investigations mentioned above deal exclusively with segregated approaches, which as mentioned, are not necessarily conservative. An alternative approach involves solving the complete system of equations in a fully coupled manner. This approach guarantees the conservation of relevant physical variables, provided careful attention is paid to the convergence criteria.  While this strategy may appear to be the most straightforward, it is not free of complications. Special care must be taken in the algorithms for solving the usually nonlinear problem, as this can easily lead to problems with convergence. Memory resources can also be prohibitive, as solving the equations in a coupled manner usually requires dealing with very big matrices.  

The DG method has also been used for simulations of combustion, mainly within a compressible framework. In the work from \textcite{johnsonConservativeDiscontinuousGalerkin2020} the compressible Navier--Stokes equations are solved using a nodal DG scheme for combustion with complex chemistry and transport parameters. An hp-adaptive method is also presented. Similarly in the work from \textcite{lvHighorderDiscontinuousGalerkin2017} a DG solver for multi-component chemically reacting flows, which solves the fully compressible Navier-Stokes equation, is presented. A hybrid-flux formulation is used, where a conservative Riemann solver is used for shock treatment, and a double-flux formulation is used in smooth regions. They show its applicability for non-reacting and reacting flows, particularly for systems characterized by high-Mach numbers. To the best of the authors' knowledge, there are no publications that address low-Mach number reactive flows using a fully coupled solver, as done in the present work.

The system of equations obtained from the discretization of highly nonlinear systems  can be very difficult to solve. Fixed-point iteration schemes have been used often used as a linearization strategy, as done by \textcite{kleinHighorderDiscontinuousGalerkin2016}. A significant limitation of this strategy is the necessity for an adequate selection of the under-relaxation factors, which are highly dependent on the specific problem. A much more robust strategy is the use of Newton type methods. Here, a problem-dependent factor is not needed. There is however a trade-off, because the Jacobian matrix has to be calculated, which can be computationally expensive. This approach has been used for combustion related problems in numerous works. \textcite{karaaPreconditionedMultigridSimulation2003} studied the axisymmetric laminar jet diffusion flame and investigated the behavior of a multi-grid solver when using different pre-conditioners with a damped Newton algorithm. \textcite{shenNewtonMethodSteady2006a} investigated the use  and efficiency of a Newton method coupled with the Bi-CGSTAB method for an axisymmetric laminar jet flame. They concluded that, in terms of computational cost, the steady-state solution is more efficiently obtained by directly solving the steady formulation of the equations, than by solving the transient Navier-Stokes equations until the steady state is reached.

\section{The XNSEC solver} \label{sec:XNSECSolver}
The XNSEC solver is mainly focused on the simulation of combustion using the low-Mach equations. The equations are solved in a fully coupled way, and the associated nonlinear problem is solved by means of a globalized Newton method. Motivated by future work, where a burning droplet is going to be simulated, the solver is embedded in the framework for \gls{XDG} solvers presented in \textcite{kummerExtendedDiscontinuousGalerkin2017}, which focuses on applications for multi-phase flows using a sharp interface approach employing a level-set method. However, in the present work only the single-phase case is treated. 
 
The fully coupled solution of the equations is realized by means of various algorithmic components
\begin{itemize}
\item A timestepping framework based on a \gls{BDF} scheme, described in \textcite{kummer2018}.
\item A Newton-Dogleg method for the solution of nonlinear problems. The Dogleg globalization increases the likelihood of finding a convergent solution even in cases where no adequate initial estimate is available. An increased computational efficiency is archived by an special method for the calculation of the Jacobian matrix. The algorithm is described in \cref{sec:Newton}
\item A homotopy continuation scheme that allows solving highly nonlinear problems by generating a series of convergent solutions. This algorithm is described in \cref{sec:HomotopyMethod}.
\item  The solution of the flame-sheet problem for initialization of a fully coupled steady-state finite-reaction rate system. The strategy is presented in \cref{ssec:MethodCombustion}
\end{itemize}

Note that even for steady state calculations, a time stepping scheme is used and the steady-state solutions are obtained by using a Implicit Euler time stepping scheme. Since the scheme is unconditionally stable (see \cref{ssec:TemporalDiscretization}), it is possible to select a very large timestep to obtain the steady state solution. In particular, a value $\Delta t = 1.7976931348623157\cdot 10^{304}$ is used, which is four orders of magnitude lower than the largest possible value of a double-precision floating-point number.
\section{Solution of the nonlinear problem}\label{sec:SolNonLinProblem}
In the early stages of development of the XNSEC solver, the BoSSS code featured a framework for the solution of nonlinear systems using Picard iterations. These proved to be useful for systems where no large nonlinearities exist. Although the use of Picard iterations was useful for solving various types of problems (particularly problems involving incompressible flows), the method did not prove to be a particularly robust method, since it requires user-defined under-relaxation parameters in order to obtain a stable algorithm. This motivated the development of the implementation of a Newton method for the resolution of the nonlinear system which will be presented in the following pages. The method was already presented in \parencite{kikkerFullyCoupledHighorder,gutierrez-jorqueraFullyCoupledHighorder2022}, and following section is largely based on it. The use of this globalized Newton method proved to be very successful for all testcases treated in the present work.  

The variational problem defined by \cref*{DiscretizedConti,DiscretizedMomentum,DiscretizedEnergy,DiscretizedMassFractions} can be cast into a more compact notation. By subtracting all terms from the right-hand sides from the terms of the left-hand sides of \crefrange{DiscretizedConti}{DiscretizedMassFractions} the problem can be written as:
Find $\myvector{U}_h \in \mathbb{V}_\myvector{k}$
\begin{equation}
	\mathcal{N}(\myvector{U}_h,\myvector{V}_h) = 0 \quad \forall \ \myvector{V}_h \in \mathbb{V}_\myvector{k} ,
	\label{eq:CompactVariational}
\end{equation}
for
$\myvector{U}_h = (p_h,\vec{u}_h, T_h, \MFVecPrima_h)$ and
$\myvector{V}_h = (q_h,\vec{v}_h, r_h, \mathbf{s}_h)$
. A basis is assumed as
$\underline{\gvec{\Phi}} = ( \gvec{\Phi}_1, \ldots , \gvec{\Phi}_L )$ of $\mathbb{V}_\myvector{k}$,
written as a row vector, with $L \coloneqq \textrm{dim}(\mathbb{V}_\myvector{k})$.
Then $\myvector{U}_h$ can be represented as
$ \myvector{U}_h =  \underline{ \gvec{\Phi} } \cdot \myvector{U} $.
The nonlinear problem (\ref{eq:CompactVariational}) can then be expressed as
\begin{equation}
	\mathcal{A}(\myvector{U}) = 0 ,
	\label{Eq:nonLinSystem}
\end{equation}
with the nonlinear function
$\mathbb{R}^L \ni \myvector{U} \mapsto \mathcal{A}(\myvector{U}) \in \mathbb{R}^L$.
The $i$-th component of $ \mathcal{A}(\myvector{U})$, can be defined by $\mathcal{N}(-,-)$ through the relation
$[\mathcal{A}(\myvector{U})]_i = \mathcal{N}( \underline{ \gvec{\Phi} } \cdot \myvector{U} , \gvec{\Phi}_i)$.

\subsection{Newton's method}
Newton's method is a very popular and well known iterative method used for finding roots of nonlinear systems. The method is particularly attractive because shows the property of having quadratic convergence sufficiently close to the solution \parencite{deuflhardNewtonMethodsNonlinear2011}. In this section the method will be briefly described. For more information the interested reader is referred to the textbook from \textcite{kelleyIterativeMethodsLinear1995}. 

Consider the linearization of \cref{Eq:nonLinSystem} around $\myvector{U}_n$,
\begin{equation}
	\mathcal{A}(\myvector{U}_{n}) +
	\partial \mathcal{A} (\myvector{U}_n) \underbrace{ ( \myvector{U}_{n+1} -  \myvector{U}_{n} ) }_{=: \myvector{s}'_n }
	= 0.
	\label{eq:LinearizedSys}
\end{equation}
Here is $\partial \mathcal{A}$ the Jacobian matrix of $\mathcal{A}$, defined as
\begin{equation}
	\partial \mathcal{A}_{ij}(\myvector{U}) \coloneqq \frac{\partial \mathcal{A}_i}{\partial U_j}(\myvector{U}).
	\label{Eq:Jacobian}
\end{equation}
By repeatedly solving \cref{eq:LinearizedSys} one obtains a standard Newton scheme for \cref{Eq:nonLinSystem}, yielding a sequence of approximate solutions $\myvector{U}_0, \myvector{U}_1, \myvector{U}_2, \ldots$ obtained from an initial guess $\myvector{U}_0$ through the iteration scheme $ \myvector{U}_{n+1} =\myvector{U}_n + \myvector{s}'_n.$
In the classical undamped Newton method, the correction step $\myvector{s}'_n$ is set to be the whole Newton-step, i.e  $\myvector{s}'_n = \myvector{s}_n$ with
\begin{equation}
	\myvector{s}_n  \coloneqq - \partial \mathcal{A}(\myvector{U}_n)^{-1}\mathcal{A}(\myvector{U}_n),
	\label{eq:NewtonStep}
\end{equation}
which is computed using a direct solver. Unfortunately, convergence of the Newton method for any starting value $\myvector{U}_0$ is not guaranteed. As a way to remedy this problem, so-called globalization methods have been developed, where basically the area of convergence of the Newton algorithm is enlarged used different kind of strategies.  

A big drawback of Newton method is the calculation is the necessity of the Jacobian matrix, since its direct calculation using usual methods can be computationally expensive. The \textit{BoSSS} framework provides an efficient algorithm for the evaluation of the Jacobian, and is presented next.

\subsection{Calculation of the Jacobian matrix} \label{ssec:EvalJacobian}
During the development process of the XNSEC solver different strategies for the calculation of the Jacobian matrix $\partial \mathcal{A}$ were developed and are shown here. 

First, it is interesting to show the relationship existing between two well known methods for solving nonlinear systems: Picard iterations and Newton's method. Note that the nonlinear problems \cref{Eq:nonLinSystem} appearing in this work have the structure
\begin{equation}
	\mathcal{A}(\myvector{U}) \coloneqq  A(\myvector{U})\myvector{U}-\myvector{b}. \label{eq:LinearForm}
\end{equation}
Thus, the Jacobian matrix \cref{Eq:Jacobian} can be written as
\begin{equation}
	\partial \mathcal{A}_{ij}(\myvector{U})  = A_{ij} + \sum_k \pfrac{A_{ik}}{U_j} U_K = A_{ij} + A'_{ij}U_K.\label{eq:JacMatrix2}
\end{equation}
Inserting \cref{eq:LinearForm} and  \cref{eq:JacMatrix2} into \cref{eq:LinearizedSys}, the linear system to be solved using Newton's method can be written as
\begin{equation}
	\underbrace{A(\myvector{U}_n)(\myvector{U}_n+\myvector{s}'_n) - \myvector{b}}_{\text{Picard system}} + A'(\myvector{U}_n)\myvector{U}_n\myvector{s}'_n = 0 \label{eq:PicardNewtonComp}
\end{equation}
This makes the relationship between the two algorithms apparent, and shows that by usind Picard iterations some terms are being neglected from the iteration scheme. Unlike Picard's method of iterations, Newton's method requires additionally the evaluation of the Jacobi matrix, which must be approximated in some way. This section shows three strategies that were used throughout the development of this work for that purpose.

\subsubsection{Ad-hoc linearization of the Jacobian matrix}
For some problems, particularly saddle-point problems, the Jacobian $A$' can be approximated fairly well by simply evaluating the operator matrix. Thus, the system to be solved for Newton Iteration yields
\begin{equation}
	A(\myvector{U}_n)(\myvector{U}_n+\myvector{s}'_n) - \myvector{b} + A(\myvector{U}_n)\myvector{U}_n\myvector{s}'_n = 0
\end{equation}
This strategy offers a computationally cheap algorithm to obtain a solution of the nonlinear problem. However, the method offers limited robustness, and is known to be prone to fail for non-saddle-point problems \parencite{kikkerHighOrderEXtendedDiscontinuous2020}. 

\subsubsection{Approximation of the Jacobian matrix by finite differences}

A straightforward way of calculating the Jacobian matrix appearing in \cref{eq:PicardNewtonComp} is to use forward finite differences as an approximation
\begin{equation}
	A'(\myvector{U})_j \coloneqq \frac{A(\myvector{U}+\varepsilon\norm{\myvector{U}}\myvector{e}_j )-A(\myvector{U})  }{\varepsilon\norm{\myvector{U}}},
\end{equation}
where $\myvector{e}_j$ is the unit vector with $j$th component equal to one, and zero in all other components. The value of $\varepsilon$ should be chosen small, as usual when calculating finite differences, but also large enough not to disturb the calculations due to problems caused by floating-point rounding calculations.  Here, value $\varepsilon = \sqrt{\mathtt{eps}}$ is selected, where $\mathtt{eps} = 2.22044604925031 \cdot 10^{-16}$ is the floating point accuracy for double precision.

The calculation of the forward finite difference approximation is a costly operation, especially for large systems, where it can be particularly prohibitive. However it offers a robust way to approximate the Jacobian. Strategies for improving the efficiency of this calculation exists, such as the use of analytic Jacobians, or use of the sparsity patterns of the Jacobian \parencite{kelleyIterativeMethodsLinear1995}. These are not treated in the present work. 
\subsubsection{Approximation of the Jacobian from differentiation of equation components}
The finite difference Jacobi matrix calculation shown above is a fairly simple but computationally expensive calculation. The \BoSSS code is capable of evaluating the Jacobian matrix automatically from the equation components given in \cref{ssec:SpatDiscretization}.
First, note that $\mathcal{A}(\myvector{U})$ could be written as
\begin{equation}
	[\mathcal{A}(\myvector{U})]_i = \mathcal{N}( \myvector{U}_h , \gvec{\Phi}_i) =
	\int_{\Omega_h}
	N_1 (\vec{x}, \myvector{U}_h , \nabla \myvector{U}_h  ) \cdot \gvec{\Phi}_i
	+ N_2 (\vec{x}, \myvector{U}_h , \nabla \myvector{U}_h  ) \cdot \nabla \gvec{\Phi}_i
	\textrm{dV}
	+
	\oint_{\Gamma} \ldots \mathrm{dS}.
	\label{eq:NonlinearGestalt}
\end{equation}
The edge integral, which is left out in \cref{eq:NonlinearGestalt},
can be expressed analogously to the volume integral, i.e. as a sum over
four nonlinear functions, multiplied by
$ \gvec{\Phi}_i^{+}$,  $\gvec{\Phi}_i^{-}$, $ \nabla \gvec{\Phi}_i^{+}$ and  $ \nabla \gvec{\Phi}_i^{-}$,
respectively.
These functions themselves may include the dependence on
$\vec{x}$, $\myvector{U}_h^{+}$,  $\myvector{U}_h^{-}$, $\nabla \myvector{U}_h^{+}$ and  $\nabla \myvector{U}_h^{-}$.
These are however omitted here for the sake of compactness, but the treatment is analogue. Realizing that 
\begin{equation}
	\frac{\partial \myvector{U}_h}{\partial \myvector{U}_j}  = \gvec{\Phi}_j
\end{equation}
and by application of the chain rule, it is possible to derive an expression for the calculation of the entries of the Jacobian matrix from the equation components as
\begin{equation}
	\partial \mathcal{A}_{ij}(\myvector{U}) =
	\int_{\Omega_h}
	( \partial_{ \myvector{U}_h}       N_1 (\vec{x}, \myvector{U}_h , \nabla \myvector{U}_h  ) \gvec{\Phi}_j
	+   \partial_{\nabla \myvector{U}_h} N_1 (\vec{x}, \myvector{U}_h , \nabla \myvector{U}_h  ) \nabla \gvec{\Phi}_j ) \cdot \gvec{\Phi}_i
	+ \ldots
	\textrm{dV}
	+
	\oint_{\Gamma} \ldots \mathrm{dS} .
	\label{eq:JacobiGestalt}
\end{equation}
All omitted  terms in \cref{eq:JacobiGestalt} can be approximated analogously to the contributions for $N_1$. In the \BoSSS code, derivatives $ \partial_{ \myvector{U}_h} N_1 ( \ldots )$ and  $ \partial_{ \nabla \myvector{U}_h} N_1 ( \ldots )$ are approximated by a finite difference, using a perturbation by $\mathtt{eps}$ in the respective argument.

This approach has the significant improvement that it offers an efficient and accurate way to obtain the Jacobian matrix, unlike the two approaches mentioned above. For this reason, this option is the one chosen for the resolution of all the testcases shown in this work.

\subsection{Newton-Dogleg Method} \label{sec:Newton}
%However, if the initial solution guess is not adequate, the method converge to a solution slowly, or may not even converge at all. A popular globalized Newton method is the Newton-Dogleg method, \textcite{pawlowskiInexactNewtonDogleg2008} which is based on a trust-region technique.

A well known problematic of Newton methods is the necessity of a good starting estimate for the algorithm to get a convergent solution. If the initial estimate is far away from a solution, the algorithm could converge but at a very slow rate and in the most critical cases it could stagnate or even diverge.  For some highly nonlinear problems this is a significant issue. The so called globalization methods are auxiliary procedures designed to alleviate  this problem. 

In general two kinds of globalization algorithms can be identified. The so called backtracking methods consist in a modification of the Newton step length (usually a reduction), in order to obtain steps that lead to a solution of the system. These are usually easy to implement, but suffer from the restriction that the Newton step direction is restricted to the one of the trial step, which could not be an ideal direction, specially if the Jacobian is ill conditioned \parencite{pawlowskiGlobalizationTechniquesNewton2006}. 

The second globalization type are the so called Trust-region globalization methods. The main idea of these kind of algorithms is to use a quadratic approximation of the function, which is assumed to be valid within a "trust region". This trust region is enlarged or reduced based on defined strategies, which improves the likelihood of finding a solution and accelerates the convergence. This kind of approaches has proven to be robust and  very useful for a variety of problems, but are usually not easy to implement. 

In this work, in order to increase robustness when the distance between $\myvector{U}_0$ and the exact solution $\myvector{U}$ is large, a trust region globalization approach known as the Newton-Dogleg method is used. The algorithm is presented by \textcite{pawlowskiGlobalizationTechniquesNewton2006,pawlowskiInexactNewtonDogleg2008}. In this section the intention is to give only the main ideas of method and refer to the original works for further details. 

Note that the exact solution of \cref{Eq:nonLinSystem} is also a minimum of the functional
\begin{equation}
	f(\myvector{U}) \coloneqq \frac{1}{2}  \left\| \mathcal{A}(\myvector{U})  \right\|^2_2 .
\end{equation}
Thus $\nabla f(\myvector{U}) = \partial \mathcal{A}(\myvector{U})^T \mathcal{A}(\myvector{U})$.
For $\myvector{U}_n$, the approximate Cauchy point with respect to the 2-norm,
is defined as the minimizer $\myvector{g}_n$ of
$ \left\|  \mathcal{A}(\myvector{U}_{n}) + \partial \mathcal{A} (\myvector{U}_n) \myvector{g}_n \right\|_2  $
in the direction of steepest decent, i.e. $\myvector{g}_n = \lambda \nabla f(\myvector{U}_n)$, $\lambda \in \mathbb{R}$.
Substituting $\myvector{w} \coloneqq - \partial \mathcal{A}(\myvector{U}_n) \nabla f(\myvector{U}_n)$, $\myvector{g}_n$ is given by
\begin{equation}
	\myvector{g}_n = \frac{\mathcal{A}(\myvector{U}_n) \cdot \myvector{w}}{\myvector{w} \cdot \myvector{w}} \nabla f(\myvector{U}_n) .
	\label{eq:CauchyPoint}
\end{equation}

In contrast to the classical undamped Newton Method, where each iteration uses the whole Newton-step $\vec{s}_n$ (cf. \cref{eq:NewtonStep}), within the Newton-Dogleg method the correction step  $\myvector{s}'_n$ is chosen along the so-called Dogleg curve, which is the piece-wise linear
curve from the origin to $\myvector{g}_n$ and further to $ \myvector{s}_n$. The selection of $\myvector{s}'_n$ on this curve is controlled by the trust-region diameter $\delta > 0$:
\begin{itemize}
	\item
	If $\|  \myvector{s}_n \|_2 \leq \delta$,  meaning that the Newton estimate lies inside the trust region, the whole Newton step is accepted and $ \myvector{s}'_n =  \myvector{s}_n$.	
	\item
	If  $\|  \myvector{g}_n \|_2 \leq \delta$ and $\|  \myvector{s}_n \|_2 > \delta$, i.e. the Newton guess is outside the trust region, but the Cauchy point is inside the trust region, 
	$\myvector{s}'_n$ is chosen on the linear interpolation from $\myvector{g}_n$ to $\myvector{s}_n$
	so that  $\|  \myvector{s}'_n \|_2 = \delta$:
	For the ansatz
	$\myvector{s}'_n = \tau \myvector{s}_n + (1-\tau) \myvector{g}_n$,
	the interpolation factor $\tau$ is given as
	$ \tau = (a^2 - c + \sqrt{(a^2 + b^2 - 2 c) \delta^2 - a^2 b^2 + c^2}) / (a^2 + b^2 - 2 c) $
	with $a = \| \myvector{g}_n \|_2$,  $b = \| \myvector{s}_n \|_2$ and $c = \myvector{g}_n \cdot \myvector{s}_n $. Thus, the new estimate is taken as point where the line connecting the Newton guess and the Cauchy point intersect the boundary of the trust region.
	
	\item 	If  $\|  \myvector{g}_n \|_2 > \delta$, both Newton guess and Cauchy point lie outside the trust region, the new iterate is taken to be the point  at  the edge of the trust region from the line connecting the actual Newton guess and the Cauchy point.
	$  \myvector{g}_n = (\delta / \|  \myvector{g}_n \|_2) \myvector{g}_n$.
\end{itemize}
The choice and adaptation of the trust region diameter $\delta$ throughout the Newton-Dogleg procedure follows a sophisticated heuristic which is for the sake of completeness described here. 

The adaptation of the trust region diameter is based on comparing the actual residual reduction $\textrm{ared}_n \coloneqq \| \mathcal{A} (\myvector{U}_n) \|_2 - \| \mathcal{A} (\myvector{U}_n  + \myvector{s}'_{n} ) \|_2$ with the predicted residual reduction
$\textrm{pred}_n \coloneqq \| \mathcal{A} (\myvector{U}_n) \|_2 - \| \mathcal{A} (\myvector{U}_n )   + \partial \mathcal{A} (\myvector{U}_n ) \myvector{s}'_{n} \|_2$. The algorithm for the adaptation of the trust region can be resumed as
\begin{itemize}
	\item[(1)]
	Set $n=0$, $\delta_n = \min(10^{10}, \max(2 \cdot 10^{-6}, \| \myvector{s}_0 \|_2 ))$.	
	\item[(2)]
	Compute the Newton step $\myvector{s}_n$ and the Cauchy point  $\myvector{g}_n$ and
	find $\myvector{s}'_n$ on the Dogleg curve with respect to the recent $\delta_n$.	
	\item[(3)]
	While $\textrm{ared}_n \leq \textrm{pred}_n$ do:
	Update trust region diameter $\delta_n \leftarrow 0.5 \ \delta_n$
	and re-compute $\myvector{s}'_n$.
	If $\delta_n < 10^{-6}$ terminate abnormally and mark the computation as failed.	
	\item[(4)]
	If the convergence criterion (see below) is fulfilled, terminate and mark the computation as success.	
	\item[(5)]
	Perform a final update of the trust region: Set
	\[
	\delta_{n+1} = \left\{ \begin{array}{ll}
		\max( 10^{-6}, \| \myvector{s}_n \|_2 ) & \text{if } \textrm{ared}_n / \textrm{pred}_n < 0.1 \text{ and } \| \myvector{s}_n \|_2 \delta_n \\
		\max( 10^{-6}, 0.25 \cdot \delta_n )    & \text{else, if } \textrm{ared}_n / \textrm{pred}_n < 0.1                                        \\
		\min( 10^{10}, 4 \cdot \delta_n )       & \text{else, if } \textrm{ared}_n / \textrm{pred}_n > 0.75                                       \\
		\delta_{n}                              & \text{otherwise}                                                                                \\
	\end{array} \right.
	\]
	Set $\myvector{U}_{n+1} = \myvector{U}_{n} + \myvector{s}'_{n} $, update $n \leftarrow n + 1$ and return to step (2).	
\end{itemize}
\subsection{Linear solver}\label{ssec:LinearSolver}

The computation of the Newton step according to \cref{eq:NewtonStep} requires the inversion of the Jacobian matrix.  For most of the problems presented in this work, the system is solved by using the package \gls{PARDISO}, originally developed by Schenk et al.  \parencite{schenkEfficientSparseLU2000,schenkTwolevelDynamicScheduling2002,schenkSolvingUnsymmetricSparse2004a},
from the ``Intel(R) Parallel Studio XE 2018 Update 3 Cluster Edition for Windows'' library collection. However for some of the testcases presented in this work, particularly the cases where combustion is present, the use of \texttt{PARDISO} for the solution of the linear problem was not possible due to memory problems. A limit of approximately 500,000 DOFs was observed. For larger problems an iterative scheme was used.

An active field of study in the BoSSS development group is that of iterative algorithms for solving linear systems. The BoSSS code features a multigrid orthonormalization  algorithm \parencite{kummerBoSSSPackageMultigrid2021}, which uses additive Schwarz schemes as smoothers for all multigrid levels, except the coarsest one, where \texttt{PARDISO} is used. This method of solution proved to be adequate to solve systems that are too big to be solved directly by \texttt{PARDISO}, and is adopted for all combustion calculations.
Additionally, BoSSS includes a matrix preconditioning procedure using LU-factorization. In particular, it allows preconditioning of each block of the matrix, with the objective of reducing the condition number of the system.

\subsection{Termination criterion} \label{ssec:TerminationCriterion}
The standard way to determine whether the nonlinear solver should be terminated is to check if the residual norm has fallen below a certain threshold, i.e.
$ \| \mathcal{A}(U_n) \| \leq \textrm{tol}  $. As noted by \textcite{pawlowskiInexactNewtonDogleg2008}, this approach is not always helpful. A universal choice of tolerance is in fact difficult, especially for investigations of the convergence properties of numerical methods. 
The reason for this is that, if the tolerance $\textrm{tol}$ is chosen to be too high, the error due to the premature termination may dominate the error of the spatial discretization and the advantages offered by the high-order methods cannot be fully employed. On the other hand, if the tolerance is set too low, the algorithm may never terminate, due to dominant numerical rounding errors.  Therefore, the goal is to continue the Newton-Dogleg method until the lowest possible limit dictated by the floating point precision is reached. 

To identify the limit in a robust way, the residual-norm skyline is defined as
\begin{equation}
	\textrm{sr}_n \coloneqq \min_{j \leq n} \| \mathcal{A}(\myvector{U}_j) \|
\end{equation}
and, for $n \geq 2$, the averaged reduction factor
\begin{equation}
	\textrm{arf}_n \coloneqq \frac{1}{2} \left(
	\frac{ \textrm{sr}_{n-2} }{  \max \{ \textrm{sr}_{n-1}, 10^{-100} \} }
	+  \frac{ \textrm{sr}_{n-1} }{  \max \{ \textrm{sr}_{n},   10^{-100} \} }
	\right) .
\end{equation}
The Newton-Dogleg method is terminated if
\begin{equation}
	n \geq 2 \text{ and }
	\textrm{sr}_n \leq 10^{-5} + 10^{-5} \| \myvector{U}_n \|_2 \text{ and }
	\textrm{arf}_n < 1.5 .
\end{equation}
The skyline approach ensures robustness against oscillations close to the lower limit. For the computations in this work, this choice guarantees that the nonlinear system is solved as accurately as possible. It secures that the numerical error is dominated by the error of the spatial or temporal discretization and not by the termination criterion of the Newton-Dogleg method. 
%\subsection{Pressure reference point} \label{ssec:PressureRefPoint}


\section{Additional convergence supporting strategies}\label{sec:ConvSupportStrat}
Although the Newton-Dogleg method works well for a variety of cases, in some of the test cases discussed in this work, convergence problems are encountered. This section shows a series of strategies used in the present work that allowed the solution of problems where large nonlinearities, or the lack of adequate initial estimates, made it difficult to obtain results by simply using the Dogleg-Method Newton. 

\subsection{Homotopy method} \label{sec:HomotopyMethod}

In some cases the problem to be solved contains some parameter that makes it difficult to obtain a result using the methods presented above. A typical example of this is a system where the Reynolds number is very high. An approach that makes it easier to obtain a solution is to choose as an initial guess a solution of a similar but simpler problem - such as the same system with a moderate Reynolds number - where even with a bad initial guess Newton's algorithm is able to obtain a solution. This strategy can be used repeatedly to gradually approach the full system. Such approaches are usually called homotopy methods (also called homotopy \textit{continuation} methods). 

The choice of the values of the parameter of the intermediate steps of the homotopy method is a point that requires attention. A straightforward option would be to manually choose the path to the solution. While such a strategy proved useful in cases such as the one presented in \parencite{klingenbergdarioDevelopmentNovelReynoldsaveraged2022} for a turbulent boundary layer flow, the requirement for user intervention to choose a suitable homotopy path makes it a not always robust solution. 

Based on the above, a methodology for homotopy methods was implemented in the BoSSS framework making use of an algorithm that automatically determines a homotopy path to the full system solution. This algorithm is based on the ideas from the textbook by  \textcite{deuflhardNewtonMethodsNonlinear2011}. In this section the main ideas of the algorithm are presented, as already shown in \parencite{gutierrez-jorqueraFullyCoupledHighorder2022}. For a more detailed explanation of the method, the interested reader is referred to the mentioned textbook.

The first step is to identify a parameter that causes difficulties in the solution of the nonlinear problem. Hereafter this variable will be referred to as the homotopy parameter $\mathrm{hp}$. 
As mentioned, the main idea of the homotopy strategy is to solve a series of simpler problems, starting with a parameter where the problem is easy to solve, and carefully increasing it until the desired value is reached.  Let  $\mathrm{Hp}$ be the value of the homotopy parameter for which a solution is sought. Let
\begin{equation}
	\mathcal{A}_{\mathrm{hp}^*}(\myvector{U}) = 0
	\label{eq:NonlinearAt-hp}
\end{equation}
be the discretized system for a certain intermediate homotopy-parameter $\mathrm{hp}^*$, between 0 and the target homotopy-parameter $\mathrm{Hp}$, i.e. $0 \leq \mathrm{hp}^* \leq \mathrm{Hp}$.
In addition, let $\myvector{U}_{\mathrm{hp},\epsilon} $ be an approximate solution to the problem (\ref{eq:NonlinearAt-hp}) with $ \mathrm{hp}^* =  \mathrm{hp}$,
up to a tolerance $\epsilon$, i.e. 
\begin{equation}
	\left\| \mathcal{A}_{\mathrm{hp}}( \myvector{U}_{\mathrm{hp},\epsilon} ) \right\|_2 \leq \epsilon .
\end{equation}
For the sake of clarity when discussing the algorithm which follows below, a distinction is made between the intermediate homotopy-parameter $\mathrm{hp}$ for which it is assumed to already have found an acceptable solution, and the next homotopy-parameter $\mathrm{hp}^*$ whose solution is being looked for.
For any $\textrm{hp}^* > \textrm{hp}$, $\epsilon= 10^{-5} \left\| \mathcal{A}_{\mathrm{hp*}}( \myvector{U}_{\mathrm{hp},\epsilon} ) \right\|_2$ is set,
meaning that the accepted solution for the intermediate homotopy parameter $\mathrm{hp}^*$ should show a residual norm reduction of at least five orders of magnitude with respect to the initial residual norm.
Finally, if $\textrm{hp}^* = \textrm{Hp}$, the termination criterion presented in section \ref{ssec:TerminationCriterion} is applied.%
\tikzexternaldisable
\begin{figure}[t]
	\centering
	\pgfplotsset{
		compat=1.3,
		tick align = outside,
		yticklabel style={/pgf/number format/fixed},
	}
	\begin{tikzpicture}
		\begin{axis}[
			set layers=standard,
			width=0.75\linewidth,
			height=6cm,
			axis y line*=left,
			ymode = log,
			xlabel=Iteration number,
			xmin = 0,
			ylabel=Residual and trust region diameter $\delta$,
			xtick = {0,2,...,36},
			ytickten = {-16,-12,...,12}
			]
			\addplot[ blue, mark =square*, mark size = 2pt] file{data/ConvergenceStory_HeatedCavity_withHomotopy/delta3.txt};\label{plot_one}
			\addplot[ orange, mark =o, mark size = 2.5pt] file{data/ConvergenceStory_HeatedCavity_withHomotopy/residuals3.txt};\label{plot_two}
		\end{axis}
		\begin{axis}[
			axis y line=right,
			width=0.75\linewidth,
			height=6cm,
			axis x line=none,
			ylabel= Homotopy parameter hp,
			xmin = 0,
			ymin = 50,
			ymax = 1100,
			legend style={at={(0.1,0.99)},anchor=north west,},
			xtick={0,5,...,35},
			ytick ={100,200,...,1000},
			]
			\addlegendimage{/pgfplots/refstyle=plot_two}\addlegendentry{$	\| \mathcal{A}_{\mathrm{hp}^*}(\myvector{U}_{n}) \|_2 $}
			\addlegendimage{/pgfplots/refstyle=plot_one}\addlegendentry{$\delta$}
			\addplot[red , mark =x, mark size = 2.5pt] file{data/ConvergenceStory_HeatedCavity_withHomotopy/reynolds3.txt};
			\addlegendentry{$\mathrm{hp}$}
		\end{axis}
	\end{tikzpicture}
	\caption[Residual, trust region diameter and homotopy parameter history  of the differentially heated cavity test case using the Homotopy strategy.]{Residual, trust region diameter and homotopy parameter history  of the differentially heated cavity test case using the Homotopy strategy. The homotopy parameter $\mathrm{hp}$ in this case is the Reynolds number. }
	\label{fig:Homotopyevolution}
\end{figure}%
\tikzexternalenable%
  An approximate solution for the target homotopy-parameter is obtained by means of the following algorithm:
\begin{itemize}
	\item[(1)]
	Set $\mathrm{hp} = 0$, i.e. start by obtaining an (approximate) solution $\myvector{U}_{0,\epsilon}$.
	
	\item[(2)]
	Search for a an increased homotopy-parameter $\mathrm{hp}^*$:
	Find the minimal $i \geq 0$ so that for
	$
	\mathrm{hp}^* = \frac{1}{2^i}(\mathrm{Hp} - \mathrm{hp}) + \mathrm{hp}
	$
	one has
	$
	\left\| \mathcal{A}_{\mathrm{hp}^*}(\myvector{U}_{\mathrm{hp},\epsilon}) \right\|_2
	\leq
	\delta_{\textrm{max}} \left\| \mathcal{A}_{\mathrm{hp}}(\myvector{U}_{\mathrm{hp},\epsilon}) \right\|_2
	$
	Here, $\delta_{\textrm{max}}$ is the maximal allowed increase of the residual for an increased
	homotopy-parameter $\mathrm{hp}^*$;  $\delta_{\textrm{max}}$ is adapted in the following steps,
	as an initial guess $ \delta_{\textrm{max}} = 10^6$ is used.
	
	\item[(3)]
	Use the Newton-Dogleg method to compute an approximate solution to the problem (\ref{eq:NonlinearAt-hp}),
	for the homotopy-parameter $\mathrm{hp}^*$,
	using the solution $\myvector{U}_{\mathrm{hp},\epsilon}$ as an initial guess.
	
	\begin{itemize}
		\item
		If the Newton-Dogleg method did not converge successfully within ten steps,
		the homotopy-parameter increase from $\mathrm{hp}$ to $\mathrm{hp}^*$ was probably too large.
		Set $\delta_{\textrm{max}} \leftarrow 0.2\delta_{\textrm{max}}$ and go to step (2).
		
		\item
		If the Newton-Dogleg method reached its convergence criterion and
		if the target homotopy-parameter is reached, i.e. $\mathrm{hp}^* = \mathrm{Hp}$,
		the algorithm has successfully found an approximate solution
		for $ \mathcal{A}_{\mathrm{Hp}}(\myvector{U}) = 0$ and can terminate.
		
		\item
		Otherwise, if the Newton-Dogleg method converged successfully, but is below the target homotopy-parameter:
		Accept the solution and set $\mathrm{hp} \leftarrow \mathrm{hp}^*$.
		If the Newton-Dogleg method took less than three iterations to reach the convergence criterion,
		set  $\delta_{\textrm{max}} \leftarrow 8\delta_{\textrm{max}}$.
		Return to step (2).
	\end{itemize}	
\end{itemize}%
%
An exemplary run of the method is shown in \cref{fig:Homotopyevolution}. The homotopy parameter $\mathrm{hp}$ in this particular case is the Reynolds number. The homotopy-parameter hp was increased for iterations 10, 18, 22 and 24, causing an increase of the residuals $\| \mathcal{A}_{\mathrm{hp}^*}(\myvector{U}_{n}) \|_2 $, leading to a convergent solution after 34 Newton iterations. The presented algorithm offers a robust method for finding steady-state solutions of highly nonlinear systems.

\subsection{Solver safeguard}
Another strategy that proved to be useful in improving the convergence properties of the iterative scheme is the use of a solver safeguard, which is used to avoid unphysical solutions during the solution procedure, such as negative temperatures or temperatures higher than the adiabatic temperature.

In particular, the basic idea is to clip values from the solution fields delivered by Newton's algorithm which are known to be unphysical, and limit the solution fields by user defined values. This clipping emulates in a sense the effect of schemes such as \gls{TVM} or \gls{ENO} \parencite{nicoudConservativeHighOrderFiniteDifference2000}. 

For an arbitrary scalar $\xi$ the values are bounded in the range $[\xi_{\text{min}} - \epsilon_{\text{safe}}, \xi_{\text{max}} + \epsilon_{\text{safe}}]$, where $\xi_{\text{min}}$ and $\xi_{\text{max}}$ are user defined bounds, and  $\epsilon_{\text{safe}} = 10^{-4}$. For example, the mass fractions by definition should have a value between zero and one, thus $Y_{k,\text{min}} = 0$ and  $Y_{k,\text{max}} = 1$. For certain problems, particularly problems involving combustion, it could be also useful to limit the value of the temperature, which can be bounded using the inlet conditions as the minimum value, and the adiabatic temperature as the maximum value.

The occurrence of these non-physical values is not always problematic, and in theory the Newton algorithm above should be able to correctly handle them and finally find the solution of the nonlinear system. However in certain cases this can lead to problems. Just to mention one example, a negative temperature would result in a imaginary value of the viscosity if it is calculated according to \cref{eq:nondim_sutherland}. Particularly for problems with sharp gradients this could be problematic due to dispersion phenomena. 

\subsection{Flame sheet estimates for steady state combustion simulations}\label{ssec:MethodCombustion}
The proposed algorithm for obtaining steady state solutions of finite reaction rate combustion problems involves first solving the problem assuming an infinite reaction rate (the flame sheet problem). This requires first solving the system presented in \cref{sec:FlameSheet}, where \cref{DiscretizedConti2,DiscretizedMomentum2,DiscretizedEnergy2} need to be solved in a coupled manner together with the expressions that link the temperature and mass fractions to the mixture fraction in order to be able to evaluate the density and transport parameters. This idea has been already employed in various works \parencite{smookeNumericalSolutionTwoDimensional1986,smookeNumericalModelingAxisymmetric1992}.

 The reason for the use of this pre-step is twofold:
\begin{itemize}
	\item Solving \cref*{eq:LowMach_Conti,eq:LowMach_Momentum,eq:LowMachEnergy,eq:LowMachMassBalance} using a Newton-type method requires adequate starting estimates in order to converge. Using the flame sheet solution as initial estimate improves the convergence properties of the method.
	\item The system of  \cref*{eq:LowMach_Conti,eq:LowMach_Momentum,eq:LowMachEnergy,eq:LowMachMassBalance} has multiple solutions. One is the pure mixing (frozen) solution, where no chemical reaction has taken place, and other one is the ignited solution, where the flame is present (see \cref{fig:Sshaped}) . Using the flame sheet solution as initial estimate makes much more likely that the path taken by Newton's algorithm will tend towards the ignited solution.
\end{itemize}
This approach proved to be very useful for finding solutions of steady state flame simulations in a variety of cases. 

One question one could certainly ask is under what flame conditions the infinite reaction rate solution (also called flame sheet solution in the following) effectively is a good initial estimate for Newton's algorithm.  Obviously for systems that respect the assumptions done for the flame sheet the obtained solution will be very close to the finite-rate solution (see \cref{fig:MixtureFraction_finiteRateComparison}). The assumption of an infinitely fast chemical reaction implies that the time scales associated with the chemical reaction are infinitely smaller than the flow scales, or in other words, $\text{Da} \to \infty$. For this reason, the flame sheet solution is expected to give a similar solution for cases close to equilibrium (where the Damköhler number is large). On the other hand, in cases that are far from equilibrium, as, for example, in the case of a flame in conditions close to extinction, it is expected that the flame sheet solution will depart considerably from the solution with a finite reaction rate.

It should be noted that within the derivation of the equations for the flame sheet it is only assumed that the heat capacity is the same for all components ($c_{kp} = c_p$), but it is still possible to consider a dependence on temperature. However, this introduces a difficulty, since the evaluation of the temperature with \cref{eq:BS-YF} requires $c_p$, which according to \cref{eq:nondim_cpmixture}, depends in turn on the temperature. Solving the system of equations required to obtain $c_p$ and $T$ is very expensive, since it would require solving it every time the temperature must be evaluated -in particular for the evaluation of the density $\rho$ and transport parameters $\mu$ and $\rho D$.  This problem can be solved by simply assuming a constant representative value of $c_p$. 

The problem that now arises is the selection of a suitable $c_p$. In the work by \textcite{xuApplicationPrimitiveVariable1993} it is suggested to estimate it simply on the basis of experimental measurements, or also by selecting some representative value, such as $c_p$ evaluated at the adiabatic temperature and stoichiometric conditions. In particular, in this work the value $\hat c_p = \SI{1.3}{\kilo \joule \per \kilo \gram \per \kelvin}$ was adequate for all calculations. This constant value of the heat capacity proved to yield a flame sheet solution which is an adequate estimate for finite-rate simulations, even for cases with a nonconstant heat capacity. 

In a similar fashion, the assumption of unity Lewis number in the flame sheet system delivers a solution that slightly deviates from the solution of the finite chemistry rate problem with nonunity Lewis numbers. Nevertheless, this small deviation does not preclude the use of the flame sheet solution as an adequate initial estimate for Newton's method. 

\subsubsection{Smoothing of the flame sheet}
\begin{figure}[h]
	\centering
	\inputtikz{SmoothingFunc}
	\caption{Smoothing function  at $z_{\text{st}} = 0.22$ for different smoothing parameters $\sigma$. }\label{fig:SmoothingFunc}
\end{figure}
It should be noted regarding the solution of the flame sheet problem (cf. \cref{sec:FlameSheet}) that the sharp change in the primitive variables around $z = z_{st}$  is problematic in certain scenarios. In particular, the non-smoothness of the derived variables could lead to Gibbs phenomenon-type problems if the stoichiometric point happens to be in a unfavourable position within a cell. This inconvenient can be remedied to a certain extent by using a regularized form of the equations. The smoothing function $\mathcal{H}$ is defined as
\begin{equation}\label{eq:regularization_MF}
	\mathcal{H}(z) \approx \frac{1}{2}(1+\tanh(\sigma(z - z_{st}) )).
\end{equation}
This function is useful for creating a smooth transition between two functions, since it returns values close to 0 for $z \ll z_{st}$ and values close to 1 for $z \gg z_{st}$. The sharpness of the transition at the point $z = z_{st}$ is dictated by the parameter $\sigma$. In \cref{fig:SmoothingFunc} the smoothing function $\mathcal{H}$ using different smoothing parameters $\sigma$ is shown. Clearly, increasing the value of $\sigma$ increases the sharpness of the transition at the point $z_{st}$. For a very big $\sigma$ value the function $	\mathcal{H}$ resembles the Heaviside step function

Using \cref{eq:regularization_MF} for creating a smooth transition of \cref{eq:BS-T,eq:BS-YF,eq:BS-YO,eq:BS-YP}, the temperature and mass fraction fields can be written as
\begin{subequations}
	\begin{align}
		T(z)   & = z T_F^0 + (1-z)T_O^0 + \frac{Q Y_F^0}{c_p} z_{st}\frac{1- z}{1-z_{st}}\mathcal{H}(z) +  \frac{Q Y_F^0}{c_p}z\left(1-\mathcal{H}(z)\right),  \label{eq:BS-TR} \\[1ex]
		Y_F(z) & = Y_F^0\frac{z - z_{st}}{1-z_{st}} \mathcal{H}(z), \label{eq:BS-YFR}                                                                                           \\[1ex]
		Y_O(z) & = Y_O^0 \frac{z_{st}-z}{z_{st}} (1-\mathcal{H}(z)), \label{eq:BS-YOR}                                                                                          \\[1ex]
		Y_P(z) & =  Y_O^0\frac{M_P\nu_P}{M_O\nu_O}(1-z)\mathcal{H}(z) +	Y_F^0\frac{M_P\nu_P}{M_F\nu_F}z (1-\mathcal{H}(z)), \label{eq:BS-YPR}                                   \\[1ex]
		Y_N(z) & = (1-Y_F^0)z + (1-Y_O^0)(1-z). \label{eq:BS-YNR}
	\end{align}
\end{subequations}
The use of this regularized form of the equations results in practice on a spreading of the flame front, which eases the numerical calculation \parencite{braackAdaptiveFiniteElement1997}.
\begin{figure}[h]
	\centering
	\inputtikz{SmoothingPicture}
	\caption{Temperature profile calculated in the center-line of a counter-flow flame configuration for different smoothing parameters $\sigma$.}
	\label{fig:smoothings}
\end{figure}
In \cref{fig:smoothings} the effect of the smoothing factor $\sigma$ on calculations of a flame in a counter-flow configuration are shown. It can be clearly observed how for decreasing $\sigma$ the solution becomes smoother.


\subsection{Adaptive Mesh Refinement}\label{ssec:MeshRefinement}
The BoSSS code offers the capability to modify throughout the simulation the numerical mesh by means of an \gls{AMR}  algorithm. In particular, the locality of a DG method allows a straightforward implementation, as the DG discretization admits the apparition of hanging nodes in the numerical mesh. The base mesh can be refined by subdividing a cell element into four elements of the same size using a quadtree-like data structure. It is also ensured that neighbouring cells always exhibit a 2:1 cell ratio on every edge \parencite{smudamartinDirectNumericalSimulation2021}. 

The mesh refinement process occurs before each time-step is started. This allows to use the \Gls{AMR} algorithm on start of the application, which can be useful in case the user needs an increased mesh resolution in particular areas of the computational domain. %Note that, in practice, the mesh-refinement procedure results in a soft-restart of the simulation. 
\begin{sloppypar}
The mesh refinement is controlled by user defined refinement-coarsening criteria. During the development of the XNSEC solver different refinement strategies where investigated and implemented. Some of them worth mentioning are the refinement strategy  $\texttt{AMRBasedOnFieldGradient}$, where the mesh is refined or coarsened based on the magnitude of the gradients of a particular solution field. For simulations where combustion is present, a sensible choice for the refinement strategy are the magnitude of temperature gradients or magnitude of mass fraction gradients. Additionally, the strategy $\texttt{AMRonFlameSheet}$ allows the refinement/coarsening around the stoichiometric surface $z = z_{st}$.
\end{sloppypar}
In \cref{fig:CoFlowMeshStrategy} the refinement of a coflowing flame configuration using the $\texttt{AMRonFlameSheet}$ strategy is shown. The mentioned strategies proved to be useful in combustion simulations, as will be discussed later.


\begin{figure}
	\centering
	\pgfplotsset{width=0.50\textwidth, compat=1.3}
	\inputtikz{MeshRefinementCoflow1}
	\inputtikz{MeshRefinementCoflow2}
	\inputtikz{MeshRefinementCoflow3}
	\caption{Adaptive mesh refinement around the stoichiometric surface in a coflow flame configuration. The iso-contour $z = z_{st}$ of the mixture fraction is shown in red.}\label{fig:CoFlowMeshStrategy}
\end{figure}
