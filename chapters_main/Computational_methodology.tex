\chapter{Computational methodology} \label{ch:CompMethodology}
%
%\chapter{State of the art}
%\section{Introduction and state of the art.}
%\subsection{Combustion}
%\subsubsection{Non-Premixed Flames}
\paragraph{State of the art of DG methods}
There are numerous works in which the DG method has been used within the context of incompressible flows. \textcite{shahbaziHighorderDiscontinuousGalerkin2007,kummerBoSSSDiscontinuousGalerkin2012,kleinSIMPLEBasedDiscontinuous2013}  However, there are not many publications in which the flow problem at low Mach numbers is addressed using a pressure-based solver. In the work by Klein et al. \textcite{kleinHighorderDiscontinuousGalerkin2016} the low-Mach equations are solved in a DG Framework making use of a SIMPLE type scheme. The solution of a time-step involves an iterative process that requires multiple matrix assemblies and solutions. The obtained systems of equations are solved by means of fixed-point iterations, where relaxation factors are necessary to obtain convergence of the computations.
In the work of Hennink et al. \textcite{henninkPressurebasedSolverLowMach2021} a pressure-based solver for low-Mach flows is presented. They solve  the mass flux instead of the velocity as the primitive variable.



\paragraph{State of the art simulation of combustion? (muy amplio))}
Simulacion de procesos de combustion haciendo uso metodos de high order. Mencionar papers donde se utiliza la flame sheet estimation para la resolución de sistemas con combustion en estado estacionario. La gran mayoria de esas publicaciones está enfocada en la resolución de sistemas caracteristicos de difusion flames tales como la counterflow flame o coflowing flame, haciendo uso de modelos quimicos complejos. 



One point that should be discussed is the method of solving the discretized system presented in \cref{NumericalMethods}. The coupling of the velocity fields with the pressure that exists in the Navier Stokes equations has been a subject of much work and interest. It is well known that for incompressible flows, the system of governing equations has the disadvantage that the continuity equation does not include a time derivative, which makes it difficult to advance the system in time using conventional methods. Traditionally this type of coupling has been solved by using projection methods such as \parencite{chorinNumericalSolutionNavierStokes1967}, or by using methods such as SIMPLE \parencite{patankarNumericalHeatTransfer1980} or PISO. These types of algorithms have the advantage of low memory consumption, and have been widely studied and used, particularly in the context of methods such as FVM. On the other hand, very little research has been done on the use of methods such as SIMPLE in conjunction with DG Methods. In the work of \textcite{kleinSIMPLEBasedDiscontinuous2013} one of the first investigations was carried out with the objective of creating a framework for solving the Navier-Stokes equations in a DG-Methods context. In addition, in \parencite{kleinHighorderDiscontinuousGalerkin2016} an extension of the previously presented method for low-Mach number flows is made. All these investigations were done using the BoSSS framework. The combination of both algorithms proved to be successful in many situations. However, as will be seen later the computational times associated with it turned out to be very high under certain conditions. This prompted the development of other strategies for solving the problem. 


Another approach is to solve the complete system of equations in a fully coupled form. In this way, a solution is obtained that satisfies all constraints imposed by the set of equations, such as the divergence-free velocity constraint imposed by the continuity equation. While this approach may appear to be the most straightforward, it is not free of complications. Special care must be taken in the algorithms for solving the (usually nonlinear) associated problem, as this can easily lead to problems with convergence. 

 

The DG method has also been used for simulations of combustion, mainly within a compressible framework. In the work from Johnson et al. \textcite{johnsonConservativeDiscontinuousGalerkin2020} the compressible Navier--Stokes equations are solved using a nodal DG scheme for combustion with complex chemistry and transport parameters. An hp-adaptive method is also presented, and shown to be useful for solving the ordinary differential equations used for describing the unsteady behaviour of the system.  Similarly in the work from \textcite{lvHighorderDiscontinuousGalerkin2017} a DG solver for multi-component chemically reacting flows, which solves the fully compressible Navier-Stokes equation, is presented. A hybrid-flux formulation is used, where a conservative Riemann solver is used for shock treatment, and a double-flux formulation is used in smooth regions. They show its applicability for non-reacting and reacting flows, particularly for systems characterized by high Mach numbers. On the other hand, the solution of combustion problems using the DG method in an incompressible framework is a topic that has not received much attention in the literature. This fact is the motivation of the present paper.

The system of equations obtained from the discretization of highly nonlinear systems  can be very difficult to solve. Fixed-point iteration schemes have been used as a linearization strategy, as done by \textcite{kleinHighorderDiscontinuousGalerkin2016} A major drawback of this approach is the highly problem dependent choice of under-relaxation factors. A much more robust strategy is the use of Newton type methods. Here, a problem-dependent factor is not needed. There is however a trade-off, because the Jacobian matrix has to be calculated, which can be computationally expensive. This approach has been used for combustion problems in numerous works. Karaa et al. \textcite{karaaPreconditionedMultigridSimulation2003} studied the axisymmetric laminar jet diffusion flame and investigated the behaviour of a multi-grid solver when using different pre-conditioners with a damped Newton algorithm. Shen et al. \textcite{shenNewtonMethodSteady2006a} investigated the use  and efficiency of a Newton method coupled with the Bi-CGSTAB method for an axisymmetric laminar jet flame. They concluded that, in terms of computational cost, the steady-state solution  is more efficiently obtained by directly solving the steady formulation of the equations, than by solving the transient Navier-Stokes equations until the steady state is reached.



Puesto que para low-Mach number flows el campo de flujo está acoplado a la temperatura y por lo tanto a la densidad, no es posible resolver las ecuaciones de una forma completamente conservativa en un framework segregado.  En \textcite{knikkerComparativeStudyHighorder2011} se hace un estudio de diferentes algoritmos para la simulacion de low-Mach number flows haciendo uso de distiontas estrategias en algoritmos segregados. 


La estrategia propuesta en el presente trabajo consiste en la resolucion completamente acoplada del sistema de ecuaciones, incluyendo en el mismo sistema todas las ecuaciones governantes discretizadas asi como los parametros de transporte y ecuacion de estado. De esta forma se logra circumvent the problems asociated with the solution of the problem using segregated approaches. Como ejemplo que motiva el desarrollo y uso de estrategia \cref{fig:RuntimeComparisonk2} se puede mostrar una comparación en los tiempos de calculo para el mismo caso haciendo uso de un fully coupled approach (XNSEC) y el algoritmo SIMPLE. Si bien ambos algoritmos permiten la simulación de low-Mach number flows, claramente el algoritmo SIMPLE requiere mucho más tiempo. Esto no es por ningun motivo un indicador de que la resolución haciendo uso de un esquema fully implicit es en general superior a una estrategia como el algoritmo SIMPLE en el contexto de DG-methods, y posiblemente una mayor optimización de los algoritmos segregados podría llevar a una mejora en los tiempos de calculo.


This section has the objective of introducing the computational strategies used in the present work for the solution of low-Mach number flows. Parts of this section are based on the work published by \textcite{kikkerFullyCoupledHighorder} and \textcite{gutierrez-jorqueraFullyCoupledHigh2022}.

\textit{BoSSS} also features a method for solving highly nonlinear problems with a homotopy strategy.
Further details on the used Newton method solver, the homotopy strategy and its implementation are given in the next sections, which are adapted from \textcite{kikkerFullyCoupledHighorder}. For information on the mentioned orthonormalization multigrid algorithm, the interested reader is referred to the work of \textcite{kummerBoSSSPackageMultigrid2021}.
First, the globalized Newton method is presented that allows the resolution of a nonlinear system of equations. In addition, comments on the termination criteria are given.


In the early stages of development of the XNSEC solver the BoSSS code featured a framework for the solution of nonlinear systems using Picard iterations. These proved to be useful for systems where no large nonlinearities exist. Although the use of Picard iterations was useful for solving various types of problems (particularly problems involving incompressible flows), the method did not prove to be a particularly robust method, since it requires user-defined under-relaxation parameters in order to obtain a stable algorithm. These parameters are highly dependent on the problem at hand and require user experience to be adequately chosen.
This motivated the development of the implementation of a Newton method for the resolution of the nonlinear system. The use of a Newton method proved to be very successful for all testcases treated in the present work. 

The Newton algorithm presented in this section is a collaborative work of the \textit{BoSSS}  code developers group and has also been used for the simulation of viscoelastic \parencite{kikkerFullyCoupledHighorder} flows, among others.


It should be noted however that this globalization strategy is still not sufficient to ensure convergence for some of the test cases presented, namely for high Rayleigh numbers for the differentially heated cavity problem. For those a homotopy strategy is used, where it is started with a low homotopy-parameter, a parameter for which the solution of the problem is not hard to find, which is gradually and carefully increased until convergence for the aimed value is reached (cf. \cref{sec:HomotopyMethod}).




\section{The XNSEC solver}
The XNSEC solver is mainly focused on the simulation of combustion using the low-Mach equations. Motivated by future work, where it is intended to simulate multiphase cases involving chemical reactions, such as a burning droplet, the solver is embedded in the framework for \gls{XDG} solvers presented in \textcite{kummerExtendedDiscontinuousGalerkin2017}, which focuses on applications for multi-phase flows using a sharp interface approach employing a level-set method.  In the present work only the single-phase case is treated. The solution of the algebraic system of equations exposed above is part of an iterative framework subdivided into several levels, which allows the fully coupled solution of the equations

\begin{itemize}
\item  The solution of the flame-sheet problem for initialization of a fully coupled steady-state finite-reaction rate system. The strategy is presented in \cref{ssec:MethodCombustion}
\item A timestepping framework based on a \gls{BDF} scheme, described in \textcite{kummer2018}.
\item A homotopy scheme that allows solving highly nonlinear problems by generating a series of convergent solutions. This algorithm is described in \cref{sec:HomotopyMethod}.
\item A Newton-Dogleg method for the solution of nonlinear problems. The Dogleg globalization increases the likelihood of finding a convergent solution even in cases where no adequate initial estimate is available. An increased computational efficiency is archived by an special method for the calculation of the Jacobian matrix. The algorithm is described in \cref{sec:Newton}
\end{itemize}

Note that even for steady state calculations, a time stepping scheme is used and the steady-state solutions are obtained by using a Implicit Euler time stepping scheme. Since the scheme is unconditionally stable (see \cref{ssec:TemporalDiscretization}), it is possible to use a very large timestep to obtain the steady state solution. In particular, a value $\Delta t = 1.7976931348623157\cdot 10^{304}$ is used, which is four orders of magnitude lower than the largest possible value of a double-precision floating-point number.
\section{Solution of the nonlinear problem}\label{sec:SolNonLinProblem}



The variational problem defined by \cref*{DiscretizedConti,DiscretizedMomentum,DiscretizedEnergy,DiscretizedMassFractions} can be cast into a more compact notation. By subtracting all terms from the right-hand sides from the terms of the left-hand sides of \crefrange{DiscretizedConti}{DiscretizedMassFractions} the problem can be written as:
Find $\myvector{U}_h \in \mathbb{V}_\myvector{k}$
\begin{equation}
	\mathcal{N}(\myvector{U}_h,\myvector{V}_h) = 0 \quad \forall \ \myvector{V}_h \in \mathbb{V}_\myvector{k} ,
	\label{eq:CompactVariational}
\end{equation}
for
$\myvector{U}_h = (p_h,\vec{u}_h, T_h, \MFVecPrima_h)$ and
$\myvector{V}_h = (q_h,\vec{v}_h, r_h, \mathbf{s}_h)$
. A basis is assumed as
$\underline{\gvec{\Phi}} = ( \gvec{\Phi}_1, \ldots , \gvec{\Phi}_L )$ of $\mathbb{V}_\myvector{k}$,
written as a row vector, with $L \coloneqq \textrm{dim}(\mathbb{V}_\myvector{k})$.
Then $\myvector{U}_h$ can be represented as
$ \myvector{U}_h =  \underline{ \gvec{\Phi} } \cdot \myvector{U} $.
The nonlinear problem (\ref{eq:CompactVariational}) can then be expressed as
\begin{equation}
	\mathcal{A}(\myvector{U}) = 0 ,
	\label{Eq:nonLinSystem}
\end{equation}
with the nonlinear function
$\mathbb{R}^L \ni \myvector{U} \mapsto \mathcal{A}(\myvector{U}) \in \mathbb{R}^L$.
The $i$-th component of $ \mathcal{A}(\myvector{U})$, can be defined by $\mathcal{N}(-,-)$ through the relation
$[\mathcal{A}(\myvector{U})]_i = \mathcal{N}( \underline{ \gvec{\Phi} } \cdot \myvector{U} , \gvec{\Phi}_i)$.

\subsection{Newton's method}
Newton's method is a very popular and well known iterative method used for finding roots of nonlinear systems. The method is particularly attractive because shows the property of having quadratic convergence sufficiently close to the solution \textcite{deuflhardNewtonMethodsNonlinear2011}. In this section the method will be briefly described. For more information the interested reader is referred to the textbook from \textcite{kelleyIterativeMethodsLinear1995}. 

Consider the linearization of \cref{Eq:nonLinSystem} around $\myvector{U}_n$,
\begin{equation}
	\mathcal{A}(\myvector{U}_{n}) +
	\partial \mathcal{A} (\myvector{U}_n) \underbrace{ ( \myvector{U}_{n+1} -  \myvector{U}_{n} ) }_{=: \myvector{s}'_n }
	= 0.
	\label{eq:LinearizedSys}
\end{equation}
Here is $\partial \mathcal{A}$ the Jacobian matrix of $\mathcal{A}$, defined as
\begin{equation}
	\partial \mathcal{A}_{ij}(\myvector{U}) \coloneqq \frac{\partial \mathcal{A}_i}{\partial U_j}(\myvector{U}).
	\label{Eq:Jacobian}
\end{equation}
By repeatedly solving \cref{eq:LinearizedSys} one obtains a standard Newton scheme for \cref{Eq:nonLinSystem}, yielding a sequence of approximate solutions $\myvector{U}_0, \myvector{U}_1, \myvector{U}_2, \ldots$ obtained from an initial guess $\myvector{U}_0$ through the iteration scheme $ \myvector{U}_{n+1} =\myvector{U}_n + \myvector{s}'_n.$
In the classical undamped Newton method, the correction step $\myvector{s}'_n$ is set to be the whole Newton-step, i.e  $\myvector{s}'_n = \myvector{s}_n$ with
\begin{equation}
	\myvector{s}_n  \coloneqq - \partial \mathcal{A}(\myvector{U}_n)^{-1}\mathcal{A}(\myvector{U}_n),
	\label{eq:NewtonStep}
\end{equation}
which is computed using a direct solver. Unfortunately, convergence of the Newton method for any starting value $\myvector{U}_0$ is not guaranteed. As a way to remedy this problem, so-called globalization methods have been developed, where basically the area of convergence of the Newton algorithm is enlarged used different kind of strategies.  

A big drawback of Newton method is the calculation of the Jacobian matrix, since its direct calculation using usual methods can be computationally expensive. The \textit{BoSSS} framework provides an efficient algorithm for the evaluation of the Jacobian, and is presented in \cref{ssec:EvalJacobian}

\subsection{Calculation of the Jacobian matrix} \label{ssec:EvalJacobian}
During the development process of the XNSEC solver different strategies for the calculation of the Jacobian matrix $\partial \mathcal{A}$ were tested and are shown here. 

First, it is interesting to show the relationship existing between two well known methods for solving nonlinear systems: Picard iterations and Newton's method. Note that the nonlinear problems \cref{Eq:nonLinSystem} appearing in this work have the structure
\begin{equation}
	\mathcal{A}(\myvector{U}) \coloneqq  A(\myvector{U})\myvector{U}-\myvector{b}. \label{eq:LinearForm}
\end{equation}
Thus, the Jacobian matrix \cref{Eq:Jacobian} can be written as
\begin{equation}
	\partial \mathcal{A}_{ij}(\myvector{U})  = A_{ij} + \sum_k \pfrac{A_{ik}}{U_j} U_K = A_{ij} + A'_{ij}U_K.\label{eq:JacMatrix2}
\end{equation}
Inserting \cref{eq:LinearForm} and  \cref{eq:JacMatrix2} into \cref{eq:LinearizedSys}, the linear system to be solved using Newton's method can be written as
\begin{equation}
	\underbrace{A(\myvector{U}_n)(\myvector{U}_n+\myvector{s}'_n) - \myvector{b}}_{\text{Picard system}} + A'(\myvector{U}_n)\myvector{U}_n\myvector{s}'_n = 0
\end{equation}
This makes the relationship between the two algorithms apparent. Unlike Picard's method of iterations, Newton's method requires additionally the evaluation of the Jacobi matrix, which must be approximated in some way. This section shows three strategies that were used throughout the development of this work for that purpose.

\subsubsection{Ad-hoc linearization of the Jacobian matrix}
For some problems, particularly saddle-point problems, the Jacobian $A$' can be approximated fairly well by simply evaluating the operator matrix. Thus, the system to be solved for Newton Iteration yields
\begin{equation}
	A(\myvector{U}_n)(\myvector{U}_n+\myvector{s}'_n) - \myvector{b} + A(\myvector{U}_n)\myvector{U}_n\myvector{s}'_n = 0
\end{equation}
This strategy offers a computationally cheap algorithm to obtain a solution of the nonlinear problem. However, the method offers limited robustness, and is known to be prone to fail for non-saddle-point problems \parencite{kikkerHighOrderEXtendedDiscontinuous2020}. 

\subsubsection{Approximation of the Jacobian matrix by finite differences}

A straightforward way of calculating the Jacobian matrix is to use forward finite differences as an approximation.

\begin{equation}
	A'(\myvector{U})_j \coloneqq \frac{A(\myvector{U}+\varepsilon\norm{\myvector{U}}\myvector{e}_j )-A(\myvector{U})  }{\varepsilon\norm{\myvector{U}}}
\end{equation}
where $\myvector{e}_j$ is the unit vector with $j$th component equal to one, and zero in all other components. The value of $\varepsilon$ should be chosen small, as usual when calculating finite differences, but also large enough not to disturb the calculations due to problems caused by floating-point rounding calculations.  For all calculations in this work, the value $\varepsilon = \sqrt{\mathtt{eps}}$ was adequate, where $\mathtt{eps} = 2.22044604925031 \cdot 10^{-16}$ is the floating point accuracy for double precision.

The calculation of the forward finite difference approximation is a costly operation, especially for large systems, where it can be particularly prohibitive. However it offers a robust way to approximate the Jacobian. Strategies for improving the efficiency of this calculation exists, such as the use of analytic Jacobians, or use of the sparsity patterns of the Jacobian \parencite{kelleyIterativeMethodsLinear1995}. These are not treated in the present work. 
\subsubsection{Approximation of the Jacobian from differentiation of equation components}
The finite difference Jacobi matrix calculation shown above is a fairly simple but computationally expensive calculation. The \BoSSS code is capable of evaluating the Jacobian matrix automatically from the equation components given in Section \ref{ssec:SpatDiscretization}.
First, note that $\mathcal{A}(\myvector{U})$ could be written as
\begin{equation}
	[\mathcal{A}(\myvector{U})]_i = \mathcal{N}( \myvector{U}_h , \gvec{\Phi}_i) =
	\int_{\Omega_h}
	N_1 (\vec{x}, \myvector{U}_h , \nabla \myvector{U}_h  ) \cdot \gvec{\Phi}_i
	+ N_2 (\vec{x}, \myvector{U}_h , \nabla \myvector{U}_h  ) \cdot \nabla \gvec{\Phi}_i
	\textrm{dV}
	+
	\oint_{\Gamma} \ldots \mathrm{dS}.
	\label{eq:NonlinearGestalt}
\end{equation}
The edge integral, which is left out in \cref{eq:NonlinearGestalt},
can be expressed analogously to the volume integral, i.e. as a sum over
four nonlinear functions, multiplied by
$ \gvec{\Phi}_i^{+}$,  $\gvec{\Phi}_i^{-}$, $ \nabla \gvec{\Phi}_i^{+}$ and  $ \nabla \gvec{\Phi}_i^{-}$,
respectively.
These functions themselves may include the dependence on
$\vec{x}$, $\myvector{U}_h^{+}$,  $\myvector{U}_h^{-}$, $\nabla \myvector{U}_h^{+}$ and  $\nabla \myvector{U}_h^{-}$.
These are however omitted here for the sake of compactness, but the treatment is analogue. Realizing that 
\begin{equation}
	\frac{\partial \myvector{U}_h}{\partial \myvector{U}_j}  = \gvec{\Phi}_j
\end{equation}
and by application of the chain rule, it is possible to derive an expression for the calculation of the entries of the Jacobian matrix from the equation components as
\begin{equation}
	\partial \mathcal{A}_{ij}(\myvector{U}) =
	\int_{\Omega_h}
	( \partial_{ \myvector{U}_h}       N_1 (\vec{x}, \myvector{U}_h , \nabla \myvector{U}_h  ) \gvec{\Phi}_j
	+   \partial_{\nabla \myvector{U}_h} N_1 (\vec{x}, \myvector{U}_h , \nabla \myvector{U}_h  ) \nabla \gvec{\Phi}_j ) \cdot \gvec{\Phi}_i
	+ \ldots
	\textrm{dV}
	+
	\oint_{\Gamma} \ldots \mathrm{dS} .
	\label{eq:JacobiGestalt}
\end{equation}
All omitted  terms in \cref{eq:JacobiGestalt} can be approximated analogously to the contributions for $N_1$. In the \BoSSS code, derivatives $ \partial_{ \myvector{U}_h} N_1 ( \ldots )$ and  $ \partial_{ \nabla \myvector{U}_h} N_1 ( \ldots )$ are approximated by a finite difference, using a perturbation by $\varepsilon$ in the respective argument.

This approach has the significant improvement that it offers an efficient and accurate way to obtain the Jacobian matrix, unlike the two approaches mentioned above. For this reason, this option is the one chosen for the resolution of all the testcases shown in this work.



\subsection{Newton-Dogleg Method} \label{sec:Newton}
%However, if the initial solution guess is not adequate, the method converge to a solution slowly, or may not even converge at all. A popular globalized Newton method is the Newton-Dogleg method, \textcite{pawlowskiInexactNewtonDogleg2008} which is based on a trust-region technique.

A well known problematic of Newton methods is the necessity of a good starting estimate for the algorithm to get a convergent solution. If the initial estimate is far away from a solution, the algorithm could converge but at a very slow rate and in the most critical cases it could stagnate or even diverge.  For some highly nonlinear problems this is a significant issue. The so called globalization methods are auxiliary procedures designed to alleviate  this problem. 

In general two kinds of globalization algorithms can be identified. The so called backtracking methods consist in a modification of the Newton step length (usually a reduction), in order to obtain steps that lead to a solution of the system. These are usually easy to implement, but suffer from the restriction that the Newton step direction is restricted to the one of the trial step, which could not be an ideal direction, specially if the Jacobian is ill conditioned \parencite{pawlowskiGlobalizationTechniquesNewton2006}. 

The second globalization type are the so called Trust-region globalization methods. The main idea of these kind of algorithms is to use a quadratic approximation of the function, which is assumed to be valid within a "trust region". This trust region is enlarged or reduced based on defined strategies, which improves the likelihood of finding a solution and accelerates the convergence. This kind of approaches has proven to be robust and  very useful for a variety of problems, but are usually not easy to implement. 

In this work, in order to increase robustness when the distance between $\myvector{U}_0$ and the exact solution $\myvector{U}$ is large, a trust region globalization approach known as the Newton-Dogleg method is used. The algorithm is presented by \textcite{pawlowskiGlobalizationTechniquesNewton2006,pawlowskiInexactNewtonDogleg2008}. In this section the intention is to give only the main ideas of method and refer to the original works for further details. 

Note that the exact solution of \cref{Eq:nonLinSystem} is also a minimum of the functional
\begin{equation}
	f(\myvector{U}) \coloneqq \frac{1}{2}  \left\| \mathcal{A}(\myvector{U})  \right\|^2_2 .
\end{equation}
Thus $\nabla f(\myvector{U}) = \partial \mathcal{A}(\myvector{U})^T \mathcal{A}(\myvector{U})$.
For $\myvector{U}_n$, the approximate Cauchy point with respect to the 2-norm,
is defined as the minimizer $\myvector{g}_n$ of
$ \left\|  \mathcal{A}(\myvector{U}_{n}) + \partial \mathcal{A} (\myvector{U}_n) \myvector{g}_n \right\|_2  $
in the direction of steepest decent, i.e. $\myvector{g}_n = \lambda \nabla f(\myvector{U}_n)$, $\lambda \in \mathbb{R}$.
Substituting $\myvector{w} \coloneqq - \partial \mathcal{A}(\myvector{U}_n) \nabla f(\myvector{U}_n)$, $\myvector{g}_n$ is given by
\begin{equation}
	\myvector{g}_n = \frac{\mathcal{A}(\myvector{U}_n) \cdot \myvector{w}}{\myvector{w} \cdot \myvector{w}} \nabla f(\myvector{U}_n) .
	\label{eq:CauchyPoint}
\end{equation}

In contrast to the classical undamped Newton Method, where each iteration used the whole Newton-step $\vec{s}_n$ (cf. \cref{eq:NewtonStep}), within the Newton-Dogleg method, the correction step  $\myvector{s}'_n$ is chosen along the so-called Dogleg curve, which is the piece-wise linear
curve from the origin to $\myvector{g}_n$ and further to $ \myvector{s}_n$. The selection of $\myvector{s}'_n$ on this curve is controlled by the trust-region diameter $\delta > 0$:
\begin{itemize}
	\item
	If $\|  \myvector{s}_n \|_2 \leq \delta$,  meaning that the Newton estimate lies inside the trust region, the whole Newton step is accepted and $ \myvector{s}'_n =  \myvector{s}_n$.	
	\item
	If  $\|  \myvector{g}_n \|_2 \leq \delta$ and $\|  \myvector{s}_n \|_2 > \delta$, i.e. the Newton guess is outside the trust region, but the Cauchy point is inside the trust region, 
	$\myvector{s}'_n$ is chosen on the linear interpolation from $\myvector{g}_n$ to $\myvector{s}_n$
	so that  $\|  \myvector{s}'_n \|_2 = \delta$:
	For the ansatz
	$\myvector{s}'_n = \tau \myvector{s}_n + (1-\tau) \myvector{g}_n$,
	the interpolation factor $\tau$ is given as
	$ \tau = (a^2 - c + \sqrt{(a^2 + b^2 - 2 c) \delta^2 - a^2 b^2 + c^2}) / (a^2 + b^2 - 2 c) $
	with $a = \| \myvector{g}_n \|_2$,  $b = \| \myvector{s}_n \|_2$ and $c = \myvector{g}_n \cdot \myvector{s}_n $. Thus, the new estimate is taken as point where the line connecting the Newton guess and the Cauchy point intersect the boundary of the trust region.
	
	\item 	If  $\|  \myvector{g}_n \|_2 > \delta$, both Newton guess and Cauchy point lie outside the trust region, the new iterate is taken to be the point  at  the edge of the trust region from the line connecting the actual Newton guess and the Cauchy point.
	$  \myvector{g}_n = (\delta / \|  \myvector{g}_n \|_2) \myvector{g}_n$.
\end{itemize}
The choice and adaptation of the trust region diameter $\delta$ throughout the Newton-Dogleg procedure follows a sophisticated heuristic. It is for the sake of completeness described here. The adaptation of the trust region diameter is based on comparing the actual residual reduction $\textrm{ared}_n \coloneqq \| \mathcal{A} (\myvector{U}_n) \|_2 - \| \mathcal{A} (\myvector{U}_n  + \myvector{s}'_{n} ) \|_2$ with the predicted residual reduction
$\textrm{pred}_n \coloneqq \| \mathcal{A} (\myvector{U}_n) \|_2 - \| \mathcal{A} (\myvector{U}_n )   + \partial \mathcal{A} (\myvector{U}_n ) \myvector{s}'_{n} \|_2$. The algorithm for the adaptation of the trust region can be resumed as
\begin{itemize}
	\item[(1)]
	Set $n=0$, $\delta_n = \min(10^{10}, \max(2 \cdot 10^{-6}, \| \myvector{s}_0 \|_2 ))$.	
	\item[(2)]
	Compute the Newton step $\myvector{s}_n$ and the Cauchy point  $\myvector{g}_n$ and
	find $\myvector{s}'_n$ on the Dogleg curve with respect to the recent $\delta_n$.	
	\item[(3)]
	While $\textrm{ared}_n \leq \textrm{pred}_n$ do:
	Update trust region diameter $\delta_n \leftarrow 0.5 \ \delta_n$
	and re-compute $\myvector{s}'_n$.
	If $\delta_n < 10^{-6}$ terminate abnormally and mark the computation as failed.	
	\item[(4)]
	If the convergence criterion (see below) is fulfilled, terminate and mark the computation as success.	
	\item[(5)]
	Perform a final update of the trust region: Set
	\[
	\delta_{n+1} = \left\{ \begin{array}{ll}
		\max( 10^{-6}, \| \myvector{s}_n \|_2 ) & \text{if } \textrm{ared}_n / \textrm{pred}_n < 0.1 \text{ and } \| \myvector{s}_n \|_2 \delta_n \\
		\max( 10^{-6}, 0.25 \cdot \delta_n )    & \text{else, if } \textrm{ared}_n / \textrm{pred}_n < 0.1                                        \\
		\min( 10^{10}, 4 \cdot \delta_n )       & \text{else, if } \textrm{ared}_n / \textrm{pred}_n > 0.75                                       \\
		\delta_{n}                              & \text{otherwise}                                                                                \\
	\end{array} \right.
	\]
	Set $\myvector{U}_{n+1} = \myvector{U}_{n} + \myvector{s}'_{n} $, update $n \leftarrow n + 1$ and return to step (2).	
\end{itemize}
\subsection{Linear solver}\label{ssec:LinearSolver}

The computation of the Newton step according to \cref{eq:NewtonStep} requires the inversion of the Jacobian matrix.  For most of the problems presented in this work, the system is solved by using  \gls{PARDISO}, originally developed by Schenk et al.  \parencite{schenkEfficientSparseLU2000,schenkTwolevelDynamicScheduling2002,schenkSolvingUnsymmetricSparse2004a},
from the ``Intel(R) Parallel Studio XE 2018 Update 3 Cluster Edition for Windows'' library collection.

However for some of the testcases presented later, particularly the cases with combustion, the use of \texttt{PARDISO} for the solution of the linear problem resulted in memory problems. A limit of approximately 500,000 DOFs was observed. 

An active field of study in the BoSSS development group is that of iterative algorithms for solving linear systems. The BoSSS code features a multigrid orthonormalization method \parencite{kummerBoSSSPackageMultigrid2021}, which uses additive Schwarz schemes as smoothers for all multigrid levels, except the coarsest one, where \texttt{PARDISO} is used. This method of solution proved to be adequate to solve systems that are too big to be solved directly by \texttt{PARDISO}, and is adopted for all combustion calculations.
Additionally, BoSSS includes a matrix preconditioning procedure using LU-factorization. In particular, it allows preconditioning of each block of the matrix, with the objective of reducing the condition number of the system.

\subsection{Termination criterion} \label{ssec:TerminationCriterion}
The usual approach to determine whether the nonlinear solver should terminate is to check whether the residual norm has fallen under a certain threshold, i.e.
$ \| \mathcal{A}(U_n) \| \leq \textrm{tol}  $. As noted by \textcite{pawlowskiInexactNewtonDogleg2008}, this approach is not always helpful. A universal choice of tolerance is in fact difficult, especially for investigations of the convergence properties of numerical methods. 
The reason for this is that, if the tolerance $\textrm{tol}$ is chosen to be too high, the error due to the premature termination may dominate the error of the spatial discretization and the advantages offered by the high-order methods cannot be fully employed. On the other hand, if the tolerance is set too low, the algorithm may never terminate, due to dominant numerical rounding errors.  Therefore, the goal is to continue the Newton-Dogleg method until the lowest possible limit dictated by the floating point precision is reached. 

To identify the limit in a robust way, the residual-norm skyline is defined as
\begin{equation}
	\textrm{sr}_n \coloneqq \min_{j \leq n} \| \mathcal{A}(\myvector{U}_j) \|
\end{equation}
and, for $n \geq 2$, the averaged reduction factor
\begin{equation}
	\textrm{arf}_n \coloneqq \frac{1}{2} \left(
	\frac{ \textrm{sr}_{n-2} }{  \max \{ \textrm{sr}_{n-1}, 10^{-100} \} }
	+  \frac{ \textrm{sr}_{n-1} }{  \max \{ \textrm{sr}_{n},   10^{-100} \} }
	\right) .
\end{equation}
The Newton-Dogleg method is terminated if
\begin{equation}
	n \geq 2 \text{ and }
	\textrm{sr}_n \leq 10^{-5} + 10^{-5} \| \myvector{U}_n \|_2 \text{ and }
	\textrm{arf}_n < 1.5 .
\end{equation}
The skyline approach ensures robustness against oscillations close to the lower limit. For the computations in this work, this choice guarantees that the nonlinear system is solved as accurately as possible. It secures that the numerical error is dominated by the error of the spatial or temporal discretization and not by the termination criterion of the Newton-Dogleg method. 
%\subsection{Pressure reference point} \label{ssec:PressureRefPoint}


\section{Additional convergence supporting strategies}\label{sec:ConvSupportStrat}
Although the Newton-Dogleg method works well for a variety of cases, in some of the test cases discussed in this work, convergence problems are encountered. This section shows a series of strategies used in the present work that allowed the solution of problems where large nonlinearities, or the lack of adequate initial estimates, made it difficult to obtain results by simply using the Dogleg-Method Newton. 

\subsection{Homotopy method} \label{sec:HomotopyMethod}

In some cases the problem to be solved contains some parameter that makes it difficult to obtain a result using the methods presented above. A typical example of this is a system where the Reynolds number is very high. An approach that makes it easier to obtain a solution is to choose as an initial guess a solution of a similar but simpler problem - such as the same system with a moderate Reynolds number - where even with a bad initial guess Newton's algorithm is able to obtain a solution. This strategy can be used repeatedly to gradually approach the full system. Such approaches are usually called homotopy methods. 

The choice of the values of the parameter of the intermediate steps of the homotopy method is a point that requires attention. A straightforward option would be to manually choose the path to the solution. While such a strategy proved useful in cases such as the one presented in \parencite{klingenbergdarioDevelopmentNovelReynoldsaveraged2022} for a turbulent boundary layer flow, the requirement for user intervention to choose a suitable homotopy path makes it a not always robust solution. 

Based on the above, a methodology for homotopy methods was implemented in the BoSSS framework making use of an algorithm that automatically determines a homotopy path to the full system solution. This algorithm is based on the ideas from the textbook by  \textcite{deuflhardNewtonMethodsNonlinear2011}. In this section the main ideas of the algorithm are presented, as already shown in \parencite{gutierrez-jorqueraFullyCoupledHigh2022}. For a more detailed explanation of the method, the interested reader is referred to the mentioned textbook.

The first step is to identify a parameter that causes difficulties in the solution of the nonlinear problem. Hereafter this variable will be referred to as the homotopy parameter. 
As mentioned, the main idea of the homotopy strategy is to solve a series of simpler problems, starting with a parameter where the problem is easy to solve, and carefully increasing it until the desired value is reached.  Let  $\mathrm{Hp}$ be the value of the homotopy parameter for which a solution is sought. Let
\begin{equation}
	\mathcal{A}_{\mathrm{hp}^*}(\myvector{U}) = 0
	\label{eq:NonlinearAt-hp}
\end{equation}
be the discretized system for a certain intermediate homotopy-parameter $\mathrm{hp}^*$, between 0 and the target homotopy-parameter $\mathrm{Hp}$, i.e. $0 \leq \mathrm{hp}^* \leq \mathrm{Hp}$.
In addition, let $\myvector{U}_{\mathrm{hp},\epsilon} $ be an approximate solution to the problem (\ref{eq:NonlinearAt-hp}) with $ \mathrm{hp}^* =  \mathrm{hp}$,
up to a tolerance $\epsilon$, i.e. 
\begin{equation}
	\left\| \mathcal{A}_{\mathrm{hp}}( \myvector{U}_{\mathrm{hp},\epsilon} ) \right\|_2 \leq \epsilon .
\end{equation}
For the sake of clarity when discussing the algorithm which follows below, a distinction is made between the intermediate homotopy-parameter $\mathrm{hp}$ for which it is assumed to already have found an acceptable solution, and the next homotopy-parameter $\mathrm{hp}^*$ whose solution is being looked for.
For any $\textrm{hp}^* > \textrm{hp}$, $\epsilon= 10^{-5} \left\| \mathcal{A}_{\mathrm{hp*}}( \myvector{U}_{\mathrm{hp},\epsilon} ) \right\|_2$ is set,
meaning that the accepted solution for the intermediate homotopy parameter $\mathrm{hp}^*$ should show a residual norm reduction of at least five orders of magnitude with respect to the initial residual norm.
Finally, if $\textrm{hp}^* = \textrm{Hp}$, the termination criterion presented in section \ref{ssec:TerminationCriterion} is applied.%
\tikzexternaldisable
\begin{figure}[t]
	\centering
	\pgfplotsset{
		compat=1.3,
		tick align = outside,
		yticklabel style={/pgf/number format/fixed},
	}
	\begin{tikzpicture}
		\begin{axis}[
			set layers=standard,
			width=0.75\linewidth,
			height=6cm,
			axis y line*=left,
			ymode = log,
			xlabel=Iteration number,
			xmin = 0,
			ylabel=Residual and trust region diameter $\delta$,
			xtick = {0,2,...,36},
			ytickten = {-16,-12,...,12}
			]
			\addplot[ blue, mark =square*, mark size = 2pt] file{data/ConvergenceStory_HeatedCavity_withHomotopy/delta3.txt};\label{plot_one}
			\addplot[ orange, mark =o, mark size = 2.5pt] file{data/ConvergenceStory_HeatedCavity_withHomotopy/residuals3.txt};\label{plot_two}
		\end{axis}
		\begin{axis}[
			axis y line=right,
			width=0.75\linewidth,
			height=6cm,
			axis x line=none,
			ylabel= Homotopy parameter hp,
			xmin = 0,
			ymin = 50,
			ymax = 1100,
			legend style={at={(0.1,0.99)},anchor=north west,},
			xtick={0,5,...,35},
			ytick ={100,200,...,1000},
			]
			\addlegendimage{/pgfplots/refstyle=plot_two}\addlegendentry{$	\| \mathcal{A}_{\mathrm{hp}^*}(\myvector{U}_{n}) \|_2 $}
			\addlegendimage{/pgfplots/refstyle=plot_one}\addlegendentry{$\delta$}
			\addplot[red , mark =x, mark size = 2.5pt] file{data/ConvergenceStory_HeatedCavity_withHomotopy/reynolds3.txt};
			\addlegendentry{$\mathrm{hp}$}
		\end{axis}
	\end{tikzpicture}
	\caption[Residual, trust region diameter and homotopy parameter history  of the differentially heated cavity test case using the Homotopy strategy.]{Residual, trust region diameter and homotopy parameter history  of the differentially heated cavity test case using the Homotopy strategy. The homotopy parameter $\mathrm{hp}$ in this case is the Reynolds number. }
	\label{fig:Homotopyevolution}
\end{figure}%
\tikzexternalenable%
 An approximate solution for the target homotopy-parameter is obtained by means of the following algorithm:
\begin{itemize}
	\item[(1)]
	Set $\mathrm{hp} = 0$, i.e. start by obtaining an (approximate) solution $\myvector{U}_{0,\epsilon}$.
	
	\item[(2)]
	Search for a an increased homotopy-parameter $\mathrm{hp}^*$:
	Find the minimal $i \geq 0$ so that for
	$
	\mathrm{hp}^* = \frac{1}{2^i}(\mathrm{Hp} - \mathrm{hp}) + \mathrm{hp}
	$
	one has
	$
	\left\| \mathcal{A}_{\mathrm{hp}^*}(\myvector{U}_{\mathrm{hp},\epsilon}) \right\|_2
	\leq
	\delta_{\textrm{max}} \left\| \mathcal{A}_{\mathrm{hp}}(\myvector{U}_{\mathrm{hp},\epsilon}) \right\|_2
	$
	Here, $\delta_{\textrm{max}}$ is the maximal allowed increase of the residual for an increased
	homotopy-parameter $\mathrm{hp}^*$;  $\delta_{\textrm{max}}$ is adapted in the following steps,
	as an initial guess $ \delta_{\textrm{max}} = 10^6$ is used.
	
	\item[(3)]
	Use the Newton-Dogleg method to compute an approximate solution to the problem (\ref{eq:NonlinearAt-hp}),
	for the homotopy-parameter $\mathrm{hp}^*$,
	using the solution $\myvector{U}_{\mathrm{hp},\epsilon}$ as an initial guess.
	
	\begin{itemize}
		\item
		If the Newton-Dogleg method did not converge successfully within ten steps,
		the homotopy-parameter increase from $\mathrm{hp}$ to $\mathrm{hp}^*$ was probably too large.
		Set $\delta_{\textrm{max}} \leftarrow 0.2\delta_{\textrm{max}}$ and go to step (2).
		
		\item
		If the Newton-Dogleg method reached its convergence criterion and
		if the target homotopy-parameter is reached, i.e. $\mathrm{hp}^* = \mathrm{Hp}$,
		the algorithm has successfully found an approximate solution
		for $ \mathcal{A}_{\mathrm{Hp}}(\myvector{U}) = 0$ and can terminate.
		
		\item
		Otherwise, if the Newton-Dogleg method converged successfully, but is below the target homotopy-parameter:
		Accept the solution and set $\mathrm{hp} \leftarrow \mathrm{hp}^*$.
		If the Newton-Dogleg method took less than three iterations to reach the convergence criterion,
		set  $\delta_{\textrm{max}} \leftarrow 8\delta_{\textrm{max}}$.
		Return to step (2).
	\end{itemize}	
\end{itemize}%
%
An exemplary run of the method is shown in \cref{fig:Homotopyevolution}. The homotopy parameter $\mathrm{hp}$ in this particular case is the Reynolds number. The homotopy-parameter hp was increased for iterations 10, 18, 22 and 24, causing an increase of the residuals $\| \mathcal{A}_{\mathrm{hp}^*}(\myvector{U}_{n}) \|_2 $, leading to a convergent solution after 34 Newton iterations. The presented algorithm offers a robust method for finding steady-state solutions of highly nonlinear systems.

\subsection{Solver safeguard}
Another strategy that proved to be useful in improving the convergence properties of the iterative scheme is the use of a solver safeguard, which is used to avoid unphysical solutions during the solution procedure, such as negative temperatures or temperatures higher than the adiabatic temperature.

In particular, the basic idea is to clip values from the solution fields delivered by Newton's algorithm which are known to be unphysical, and limit the solution fields by user defined values. This clipping emulates in a sense the effect of schemes such as \gls{TVM} or \gls{ENO} \parencite{nicoudConservativeHighOrderFiniteDifference2000}. 

For an arbitrary scalar $\xi$ the values are bounded in the range $[\xi_{\text{min}} - \epsilon_{\text{safe}}, \xi_{\text{max}} + \epsilon_{\text{safe}}]$, where $\xi_{\text{min}}$ and $\xi_{\text{max}}$ are user defined bounds, and  $\epsilon_{\text{safe}} = 10^{-4}$. For example, the mass fractions by definition should have a value between zero and one, thus $Y_{k,\text{min}} = 0$ and  $Y_{k,\text{max}} = 1$. For certain problems, particularly problems involving combustion, it could be also useful to limit the value of the temperature, which can be bounded using the inlet conditions as the minimum value, and the adiabatic temperature as the maximum value.

The occurrence of these non-physical values is not always problematic, and in theory the Newton algorithm above should be able to correctly handle them and finally find the solution of the nonlinear system. However in certain cases this can lead to problems. Just to mention one example, a negative temperature would result in a imaginary value of the viscosity if it is calculated according to \cref{eq:nondim_sutherland}. Particularly for problems with sharp gradients this could be problematic due to dispersion phenomena. 

\subsection{Flame sheet estimates for steady state combustion simulations}\label{ssec:MethodCombustion}
The proposed algorithm for obtaining steady state solutions of finite reaction rate combustion problems involves first solving the problem assuming an infinite reaction rate (the flame sheet problem). This requires first solving the system presented in \cref{sec:FlameSheet}, where \cref{DiscretizedConti2,DiscretizedMomentum2,DiscretizedEnergy2} need to be solved in a coupled manner together with the expressions that link the temperature and mass fractions to the mixture fraction in order to be able to evaluate the density and transport parameters. This idea has been already employed in various works \parencite{smookeNumericalSolutionTwoDimensional1986,smookeNumericalModelingAxisymmetric1992}.

 The reason for the use of this pre-step is twofold:
\begin{itemize}
	\item Solving \cref*{eq:LowMach_Conti,eq:LowMach_Momentum,eq:LowMachEnergy,eq:LowMachMassBalance} using a Newton-type method requires adequate starting estimates in order to converge. Using the flame sheet solution as initial estimate improves the convergence properties of the method.
	\item The system of  \cref*{eq:LowMach_Conti,eq:LowMach_Momentum,eq:LowMachEnergy,eq:LowMachMassBalance} has multiple solutions. One is the pure mixing (frozen) solution, where no chemical reaction has taken place, and other one is the ignited solution, where the flame is present (see \cref{fig:Sshaped}) . Using the flame sheet solution as initial estimate makes much more likely that the path taken by Newton's algorithm will tend towards the ignited solution.
\end{itemize}
This approach proved to be very useful for finding solutions of steady state flame simulations in a variety of cases. 

One question one could certainly ask is under what flame conditions the infinite reaction rate solution (also called flame sheet solution in the following) effectively is a good initial estimate for Newton's algorithm.  Obviously for systems that respect the assumptions done for the flame sheet the obtained solution will be very close to the finite-rate solution (see \cref{fig:MixtureFraction_finiteRateComparison}). The assumption of an infinitely fast chemical reaction implies that the time scales associated with the chemical reaction are infinitely smaller than the flow scales, or in other words, $\text{Da} \to \infty$. For this reason, the flame sheet solution is expected to give a similar solution for cases close to equilibrium (where the Damköhler number is large). On the other hand, in cases that are far from equilibrium, as, for example, in the case of a flame in conditions close to extinction, it is expected that the flame sheet solution will depart considerably from the solution with a finite reaction rate.

It should be noted that within the derivation of the equations for the flame sheet it is only assumed that the heat capacity is the same for all components ($c_{kp} = c_p$), but it is still possible to consider a dependence on temperature. However, this introduces a difficulty, since the evaluation of the temperature with \cref{eq:BS-YF} requires $c_p$, which according to \cref{eq:nondim_cpmixture}, depends in turn on the temperature. Solving the system of equations required to obtain $c_p$ and $T$ is very expensive, since it would require solving it every time the temperature must be evaluated -in particular for the evaluation of the density $\rho$ and transport parameters $\mu$ and $\rho D$.  This problem can be solved by simply assuming a constant representative value of $c_p$. 

The problem that now arises is the selection of a suitable $c_p$. In the work by \textcite{xuApplicationPrimitiveVariable1993} it is suggested to estimate it simply on the basis of experimental measurements, or also by selecting some representative value, such as $c_p$ evaluated at the adiabatic temperature and stoichiometric conditions. In particular, in this work the value $\hat c_p = \SI{1.3}{\kilo \joule \per \kilo \gram \per \kelvin}$ was adequate for all calculations. This constant value of the heat capacity proved to yield a flame sheet solution which is an adequate estimate for finite-rate simulations, even for cases with a nonconstant heat capacity. 

In a similar fashion, the assumption of unity Lewis number in the flame sheet system delivers a solution that slightly deviates from the solution of the finite chemistry rate problem with nonunity Lewis numbers. Nevertheless, this small deviation does not preclude the use of the flame sheet solution as an adequate initial estimate for Newton's method. 

\subsubsection{Smoothing of the flame sheet}
\begin{figure}[h]
	\centering
	\inputtikz{SmoothingFunc}
	\caption{Smoothing function  at $z_{\text{st}} = 0.22$ for different smoothing parameters $\sigma$. }\label{fig:SmoothingFunc}
\end{figure}
It should be noted regarding the solution of the flame sheet problem (cf. \cref{sec:FlameSheet}) that the sharp change in the primitive variables around $z = z_{st}$  is problematic in certain scenarios. In particular, the non-smoothness of the derived variables could lead to Gibbs phenomenon-type problems if the stoichiometric point happens to be in a unfavourable position within a cell. This inconvenient can be remedied to a certain extent by using a regularized form of the equations. The smoothing function $\mathcal{H}$ is defined as
\begin{equation}\label{eq:regularization_MF}
	\mathcal{H}(z) \approx \frac{1}{2}(1+\tanh(\sigma(z - z_{st}) )).
\end{equation}
This function is useful for creating a smooth transition between two functions, since it returns values close to 0 for $z \ll z_{st}$ and values close to 1 for $z \gg z_{st}$. The sharpness of the transition at the point $z = z_{st}$ is dictated by the parameter $\sigma$. In \cref{fig:SmoothingFunc} the smoothing function $\mathcal{H}$ using different smoothing parameters $\sigma$ is shown. Clearly, increasing the value of $\sigma$ increases the sharpness of the transition at the point $z_{st}$. For a very big $\sigma$ value the function $	\mathcal{H}$ resembles the Heaviside step function

Using \cref{eq:regularization_MF} for creating a smooth transition of \cref{eq:BS-T,eq:BS-YF,eq:BS-YO,eq:BS-YP}, the temperature and mass fraction fields can be written as
\begin{subequations}
	\begin{align}
		T(z)   & = z T_F^0 + (1-z)T_O^0 + \frac{Q Y_F^0}{c_p} z_{st}\frac{1- z}{1-z_{st}}\mathcal{H}(z) +  \frac{Q Y_F^0}{c_p}z\left(1-\mathcal{H}(z)\right),  \label{eq:BS-TR} \\[1ex]
		Y_F(z) & = Y_F^0\frac{z - z_{st}}{1-z_{st}} \mathcal{H}(z), \label{eq:BS-YFR}                                                                                           \\[1ex]
		Y_O(z) & = Y_O^0 \frac{z_{st}-z}{z_{st}} (1-\mathcal{H}(z)), \label{eq:BS-YOR}                                                                                          \\[1ex]
		Y_P(z) & =  Y_O^0\frac{M_P\nu_P}{M_O\nu_O}(1-z)\mathcal{H}(z) +	Y_F^0\frac{M_P\nu_P}{M_F\nu_F}z (1-\mathcal{H}(z)), \label{eq:BS-YPR}                                   \\[1ex]
		Y_N(z) & = (1-Y_F^0)z + (1-Y_O^0)(1-z). \label{eq:BS-YNR}
	\end{align}
\end{subequations}
The use of this regularized form of the equations results in practice on a spreading of the flame front, which eases the numerical calculation \parencite{braackAdaptiveFiniteElement1997}.
\begin{figure}[h]
	\centering
	\inputtikz{SmoothingPicture}
	\caption{Temperature profile calculated in the center-line of a counter-flow flame configuration for different smoothing parameters $\sigma$.}
	\label{fig:smoothings}
\end{figure}
In \cref{fig:smoothings} the effect of the smoothing factor $\sigma$ on calculations of a flame in a counter-flow configuration are shown. It can be clearly observed how for decreasing $\sigma$ the solution becomes smoother.


\subsection{Adaptive Mesh Refinement}\label{ssec:MeshRefinement}
The BoSSS code offers the capability to modify throughout the simulation the numerical mesh by means of an \gls{AMR}  algorithm. In particular, the locality of a DG method allows a straightforward implementation, as the DG discretization admits the apparition of hanging nodes in the numerical mesh. The base mesh can be refined by subdividing a cell element into four elements of the same size using a quadtree-like data structure. It is also ensured that neighbouring cells always exhibit a 2:1 cell ratio on every edge \parencite{smudamartinDirectNumericalSimulation2021}. 

The mesh refinement process occurs before each time-step is started. This allows to use the Adaptive Mesh Refinement algorithm on start of the application, which can be useful in case the user needs an increased mesh resolution in particular areas of the computational domain. Note that, in practice, the mesh-refinement procedure results in a soft-restart of the simulation. 
\begin{sloppypar}
The mesh refinement is controlled by user defined refinement-coarsening criteria. During the development of the XNSEC solver different refinement strategies where investigated and implemented. Some of them worth mentioning are the refinement strategy  $\texttt{AMRBasedOnFieldGradient}$, where the mesh is refined or coarsened based on the magnitude of the gradients of a particular solution field. For simulations where combustion is present, a sensible choice for the refinement strategy are the magnitude of temperature gradients or magnitude of mass fraction gradients. Additionally, the strategy $\texttt{AMRonFlameSheet}$ allows the refinement/coarsening around the stoichiometric surface $z = z_{st}$.
\end{sloppypar}
In \cref{fig:CoFlowMeshStrategy} the refinement of a coflowing flame configuration using the $\texttt{AMRonFlameSheet}$ strategy is shown. The mentioned strategies proved to be useful in combustion simulations, as will be discussed later.


\begin{figure}
	\centering
	\pgfplotsset{width=0.50\textwidth, compat=1.3}
	\inputtikz{MeshRefinementCoflow1}
	\inputtikz{MeshRefinementCoflow2}
	\inputtikz{MeshRefinementCoflow3}
	\caption{Adaptive mesh refinement around the stoichiometric surface in a coflow flame configuration. The iso-contour $z = z_{st}$ of the mixture fraction is shown in red.}\label{fig:CoFlowMeshStrategy}
\end{figure}
