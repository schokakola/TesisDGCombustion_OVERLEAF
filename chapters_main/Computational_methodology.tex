\chapter{Computational methodology} \label{sec:CompMethodology}


This section has the objective of introducing the computational strategies used in the present work for the solution of low-Mach number flows. Parts of this section are based on the work published by \textcite{kikkerFullyCoupledHighorder} and \textcite{gutierrez-jorqueraFullyCoupledHigh2022}.

\textit{BoSSS} also features a method for solving highly nonlinear problems with a homotopy strategy.
Further details on the used Newton method solver, the homotopy strategy and its implementation are given in the next sections, which are adapted from \textcite{kikkerFullyCoupledHighorder}. For information on the mentioned orthonormalization multigrid algorithm we refer the interested reader to the work of \textcite{kummerBoSSSPackageMultigrid2021}.
First, the globalized Newton method is presented that allows the resolution of a nonlinear system of equations. In addition, comments on the termination criteria are given.

The newton algorithm presented in this section is a collaborative work of the \textit{BoSSS}  code developers group and has also been used for the simulation of viscoelastic \parencite{kikkerFullyCoupledHighorder} flows, among others.


In the early stages of development of the XNSEC solver the BoSSS code featured a framework for the solution of nonlinear systems using Picard iterations. These proved to be useful for systems where no large nonlinearities exist. Although the use of Picard iterations was useful for solving various types of problems (particularly problems involving incompressible flows), the method did not prove to be a particularly robust method, since it requires user-defined under-relaxation parameters in order to obtain a stable algorithm. These parameters are highly dependent on the problem at hand and require user experience to be adequately chosen.
This motivated the development of the implementation of a Newton method for the resolution of the non-linear system. The use of a newton method proved to be very successful for all testcases treated in the present work. 

We note however that this globalization strategy is still not sufficient to ensure convergence for some of the test cases presented, namely for high Rayleigh numbers for the differentially heated cavity problem. For those cases we use a homotopy strategy, where we start with a low homotopy-parameter, a parameter for which the solution of the problem is not hard to find, which is gradually and carefully increased until convergence for the aimed value is reached (cf. \cref{sec:HomotopyMethod}).




The solution algorithm developed for this work consists in various ingredients

Newton solver
Efficient Calculation of the Jacobian matrix
Homotopy strategy
Initialization strategy for steady state combustion problems.
\section{Solver structure}

TODO: Steady state calculations are obtained by using a Implicit Euler timesteping scheme. Since the scheme is unconditionally stable (see \cref{ssec:TemporalDiscretization}), it is possible to use a very large timestep to obtain the steady state solution. In particular, a value $\Delta t = 1.7976931348623157E\cdot 10^{304}$ is used, which is four orders of magnitude lower than the largest possible value of a double-precision floating-point number.
\section{Solution of the nonlinear problem}



The variational problem defined by \cref*{DiscretizedConti,DiscretizedMomentum,DiscretizedEnergy,DiscretizedMassFractions} can be cast into a more compact notation. By subtracting all terms from the right-hand sides from the terms of the left-hand sides of \crefrange{DiscretizedConti}{DiscretizedMassFractions} the problem can be written as:
Find $\myvector{U}_h \in \mathbb{V}_\myvector{k}$
\begin{equation}
	\mathcal{N}(\myvector{U}_h,\myvector{V}_h) = 0 \quad \forall \ \myvector{V}_h \in \mathbb{V}_\myvector{k} ,
	\label{eq:CompactVariational}
\end{equation}
for
$\myvector{U}_h = (p_h,\vec{u}_h, T_h, \MFVecPrima_h)$ and
$\myvector{V}_h = (q_h,\vec{v}_h, r_h, \mathbf{s}_h)$
. A basis is assumed as
$\underline{\gvec{\Phi}} = ( \gvec{\Phi}_1, \ldots , \gvec{\Phi}_L )$ of $\mathbb{V}_\myvector{k}$,
written as a row vector, with $L \coloneqq \textrm{dim}(\mathbb{V}_\myvector{k})$.
Then $\myvector{U}_h$ can be represented as
$ \myvector{U}_h =  \underline{ \gvec{\Phi} } \cdot \myvector{U} $.
The nonlinear problem (\ref{eq:CompactVariational}) can then be expressed as
\begin{equation}
	\mathcal{A}(\myvector{U}) = 0 ,
	\label{Eq:nonLinSystem}
\end{equation}
with the nonlinear function
$\mathbb{R}^L \ni \myvector{U} \mapsto \mathcal{A}(\myvector{U}) \in \mathbb{R}^L$.
The $i$-th component of $ \mathcal{A}(\myvector{U})$, can be defined by $\mathcal{N}(-,-)$ through the relation
$[\mathcal{A}(\myvector{U})]_i = \mathcal{N}( \underline{ \gvec{\Phi} } \cdot \myvector{U} , \gvec{\Phi}_i)$.

\subsection{Newton's method}
Newton's method is a very popular and well known iterative method used for finding roots of nonlinear systems. The method is particularly attractive because under certain conditions it can exhibit quadratic convergence \parencite{deuflhardNewtonMethodsNonlinear2011}. In this section the method will be briefly described. For more information the interested reader is referred to the textbook from \textcite{kelleyIterativeMethodsLinear1995}. 

Consider the linearization of \cref{Eq:nonLinSystem} around $\myvector{U}_n$,
\begin{equation}
	\mathcal{A}(\myvector{U}_{n}) +
	\partial \mathcal{A} (\myvector{U}_n) \underbrace{ ( \myvector{U}_{n+1} -  \myvector{U}_{n} ) }_{=: \myvector{s}'_n }
	= 0.
	\label{eq:LinearizedSys}
\end{equation}
Here is $\partial \mathcal{A}$ the Jacobian matrix of $\mathcal{A}$, defined as
\begin{equation}
	\partial \mathcal{A}_{ij}(\myvector{U}) \coloneqq \frac{\partial \mathcal{A}_i}{\partial U_j}(\myvector{U}).
	\label{Eq:Jacobian}
\end{equation}
By repeatedly solving \cref{eq:LinearizedSys} one obtains a standard Newton scheme for \cref{Eq:nonLinSystem}, yielding a sequence of approximate solutions $\myvector{U}_0, \myvector{U}_1, \myvector{U}_2, \ldots$ obtained from an initial guess $\myvector{U}_0$ through the iteration scheme $ \myvector{U}_{n+1} =\myvector{U}_n + \myvector{s}'_n.$
In the classical undamped Newton method, the correction step $\myvector{s}'_n$ is set to be the whole Newton-step, i.e  $\myvector{s}'_n = \myvector{s}_n$ with
\begin{equation}
	\myvector{s}_n  \coloneqq - \partial \mathcal{A}(\myvector{U}_n)^{-1}\mathcal{A}(\myvector{U}_n),
	\label{eq:NewtonStep}
\end{equation}
which is computed using a direct solver.
Unfortunately, convergence of the Newton method for any starting value $\myvector{U}_0$ is not guaranteed.

A big drawback of Newton method is the calculation of the Jacobian matrix, since its direct calculation using usual methods can be computationally expensive. The \textit{BoSSS} framework provides an efficient algorithm for the evaluation of the Jacobian, and is presented in \cref{ssec:EvalJacobian}


\subsection{Dogleg Method} \label{sec:newton}

In order to increase robustness when the distance between $\myvector{U}_0$ and the exact solution $\myvector{U}$ is large,
we employ a globalization approach, presented by Pawlowski et al. \textcite{pawlowskiGlobalizationTechniquesNewton2006,pawlowskiInexactNewtonDogleg2008},
known as the Dogleg-method, or Newton-Dogleg method.
Here, we intend to give only the central ideas of method and refer to the original works for further details.
Obviously, the exact solution of \cref{Eq:nonLinSystem} is also a minimum of the functional
\begin{equation}
	f(\myvector{U}) \coloneqq \frac{1}{2}  \left\| \mathcal{A}(\myvector{U})  \right\|^2_2 .
\end{equation}
One observes that $\nabla f(\myvector{U}) = \partial \mathcal{A}(\myvector{U})^T \mathcal{A}(\myvector{U})$.
For $\myvector{U}_n$, the approximate Cauchy point with respect to the 2-norm,
is defined as the minimizer $\myvector{g}_n$ of
$ \left\|  \mathcal{A}(\myvector{U}_{n}) + \partial \mathcal{A} (\myvector{U}_n) \myvector{g}_n \right\|_2  $
in the direction of steepest decent, i.e. $\myvector{g}_n = \lambda \nabla f(\myvector{U}_n)$, $\lambda \in \mathbb{R}$.
Substituting $\myvector{w} \coloneqq - \partial \mathcal{A}(\myvector{U}_n) \nabla f(\myvector{U}_n)$, $\myvector{g}_n$ is given by
\begin{equation}
	\myvector{g}_n = \frac{\mathcal{A}(\myvector{U}_n) \cdot \myvector{w}}{\myvector{w} \cdot \myvector{w}} \nabla f(\myvector{U}_n) .
	\label{eq:CauchyPoint}
\end{equation}

For the Newton-Dogleg method, the correction step  $\myvector{s}'_n$
is chosen along the so-called Dogleg curve, which is the piece-wise linear
curve from the origin to $\myvector{g}_n$ and further to $ \myvector{s}_n$.
The selection of $\myvector{s}'_n$ on this curve is determined by the trust-region diameter $\delta > 0$:
\begin{itemize}
	\item
	If $\|  \myvector{s}_n \|_2 \leq \delta$, $ \myvector{s}'_n =  \myvector{s}_n$.
	
	\item
	If  $\|  \myvector{g}_n \|_2 \leq \delta$ and $\|  \myvector{s}_n \|_2 > \delta$,
	$\myvector{s}'_n$ is chosen on the linear interpolation from $\myvector{g}_n$ to $\myvector{s}_n$
	so that  $\|  \myvector{s}'_n \|_2 = \delta$:
	For the ansatz
	$\myvector{s}'_n = \tau \myvector{s}_n + (1-\tau) \myvector{g}_n$,
	the interpolation factor $\tau$ is given as
	$ \tau = (a^2 - c + \sqrt{(a^2 + b^2 - 2 c) \delta^2 - a^2 b^2 + c^2}) / (a^2 + b^2 - 2 c) $
	with $a = \| \myvector{g}_n \|_2$,  $b = \| \myvector{s}_n \|_2$ and $c = \myvector{g}_n \cdot \myvector{s}_n $.
	
	\item
	If  $\|  \myvector{g}_n \|_2 > \delta$,
	$  \myvector{g}_n = (\delta / \|  \myvector{g}_n \|_2) \myvector{g}_n$.
\end{itemize}
The choice and adaptation of the trust region diameter $\delta$
throughout the Newton-Dogleg procedure follows
a sophisticated heuristic,
mainly based on comparing the actual residual reduction
$\textrm{ared}_n \coloneqq \| \mathcal{A} (\myvector{U}_n) \|_2 - \| \mathcal{A} (\myvector{U}_n  + \myvector{s}'_{n} ) \|_2$
with the predicted residual reduction
$\textrm{pred}_n \coloneqq \| \mathcal{A} (\myvector{U}_n) \|_2 - \| \mathcal{A} (\myvector{U}_n )   + \partial \mathcal{A} (\myvector{U}_n ) \myvector{s}'_{n} \|_2$; For the direct solver used in this work $\textrm{pred}_n$ simplifies to
$\textrm{pred}_n \coloneqq \| \mathcal{A} (\myvector{U}_n) \|_2$.
We replicate the algorithm here, for the sake of completeness:
\begin{itemize}
	\item[(1)]
	Set $n=0$, $\delta_n = \min(10^{10}, \max(2 \cdot 10^{-6}, \| \myvector{s}_0 \|_2 ))$.
	
	\item[(2)]
	Compute the Newton step $\myvector{s}_n$ and the Cauchy point  $\myvector{g}_n$ and
	find $\myvector{s}'_n$ on the Dogleg curve with respect to the recent $\delta_n$.
	
	\item[(3)]
	While $\textrm{ared}_n \leq \textrm{pred}_n$ do:
	Update trust region diameter $\delta_n \leftarrow 0.5 \ \delta_n$
	and re-compute $\myvector{s}'_n$.
	If $\delta_n < 10^{-6}$ terminate abnormally and mark the computation as failed.
	
	\item[(4)]
	If the convergence criterion (see below) is fulfilled, terminate and mark the computation as success.
	
	\item[(5)]
	Perform a final update of the trust region: Set
	\[
	\delta_{n+1} = \left\{ \begin{array}{ll}
		\max( 10^{-6}, \| \myvector{s}_n \|_2 ) & \text{if } \textrm{ared}_n / \textrm{pred}_n < 0.1 \text{ and } \| \myvector{s}_n \|_2 \delta_n \\
		\max( 10^{-6}, 0.25 \cdot \delta_n )    & \text{else, if } \textrm{ared}_n / \textrm{pred}_n < 0.1                                        \\
		\min( 10^{10}, 4 \cdot \delta_n )       & \text{else, if } \textrm{ared}_n / \textrm{pred}_n > 0.75                                       \\
		\delta_{n}                              & \text{otherwise}                                                                                \\
	\end{array} \right.
	\]
	Set $\myvector{U}_{n+1} = \myvector{U}_{n} + \myvector{s}'_{n} $, update $n \leftarrow n + 1$ and return to step (2).	
\end{itemize}
All constants used in the algorithm above have been taken from the work of Pawlowski et al. \textcite{pawlowskiGlobalizationTechniquesNewton2006}
For a detailed description of the underlying ideas we also refer to these works,
which in turn are based on algorithms from Dennis and Schnabel's textbook. \textcite{dennisNumericalMethodsUnconstrained1996}

\subsection{Linear solver}\label{ssec:LinearSolver}
The computation of the Newton step according to \cref{eq:NewtonStep} requires the inversion of the Jacobi matrix. This is done by means of the in \BoSSS integrated orthonormalization multigrid algorithm \textcite{kummerBoSSSPackageMultigrid2021}, which at the lowest multigrid levels makes use of the sparse direct solver \texttt{PARDISO} (Parallel Sparse Direct and Multi-Recursive Iterative Linear Solvers), originally developed by Schenk et al.  \parencite{schenkEfficientSparseLU2000,schenkTwolevelDynamicScheduling2002,schenkSolvingUnsymmetricSparse2004a},
from the ``Intel(R) Parallel Studio XE 2018 Update 3 Cluster Edition for Windows'' library collection to solve the linear system. 


For some of the testcases presented here, particularly the cases with combustion, the use of \texttt{PARDISO} for the solution of the linear problem resulted in memory problems. 
An active field of study in the BoSSS development group is that of iterative algorithms for solving linear systems. The BoSSS code features a multigrid orthonormalization method, using additive Schwarz schemes as smoothers for all multigrid levels, except the coarsest one, where \texttt{PARDISO} is used. This method of solution proved to be adequate to solve systems that are too big to be solved directly by \texttt{PARDISO}, and is adopted for all combustion calculations.

\todo[inline]{What do i exactly do with the linear system? Preconditioning?}

\subsection{Calculation of the Jacobian matrix} \label{ssec:EvalJacobian}
During the development process of the XNSEC solver different strategies for the calculation of the Jacobian matrix $\partial \mathcal{A}$ were tested and are shown here. 

First, it is interesting to show the relationship existing between two well known methods for solving nonlinear systems: Picard iterations and Newton's method. First, note that the non-linear problems \cref{Eq:nonLinSystem} appearing in this work have the structure
\begin{equation}
	\mathcal{A}(\myvector{U}) \coloneqq  A(\myvector{U})\myvector{U}-\myvector{b}.
\end{equation}
Thus, the Jacobian matrix \cref{Eq:Jacobian} can be written as
\begin{equation}
	\partial \mathcal{A}_{ij}(\myvector{U})  = A_{ij} + \sum_k \pfrac{A_{ik}}{U_j} U_K = A_{ij} + A'_{ij}U_K.
\end{equation}
Inserting these , the linear system to be solved using Newton's method can be written as
\begin{equation}
	\underbrace{A(\myvector{U}_n)(\myvector{U}_n+\myvector{s}'_n) - \myvector{b}}_{\text{Picard system}} + A'(\myvector{U}_n)\myvector{U}_n\myvector{s}'_n = 0
\end{equation}
This makes the relationship between the two algorithms apparent. Unlike Picard's method of iterations, Newton's method requires additionally the evaluation of the Jacobi matrix, which must be approximated in some way. This section shows three strategies that were used throughout the development of this work for that purpose.

\subsubsection{Ad-hoc linearization of the Jacobian matrix}
For some problems, particularly saddle-point problems, the Jacobian A' can be approximated fairly well by simply evaluating the operator matrix. Thus, the system to be solved for Newton Iteration yields
\begin{equation}
	A(\myvector{U}_n)(\myvector{U}_n+\myvector{s}'_n) - \myvector{b} + A(\myvector{U}_n)\myvector{U}_n\myvector{s}'_n = 0
\end{equation}
This strategy offers a computationally cheap algorithm to obtain a solution of the nonlinear problem. However, the method offers limited robustness, and is known to be prone to fail for non-saddle-point problems \parencite{kikkerHighOrderEXtendedDiscontinuous2020}. 

\subsubsection{Approximation of the Jacobian matrix by finite differences}

A straightforward way of calculating the Jacobian matrix is to use forward finite differences as an approximation.

\begin{equation}
	A'(\myvector{U})_j \coloneqq \frac{A(\myvector{U}+\varepsilon\norm{\myvector{U}}\myvector{e}_j )-A(\myvector{U})  }{\varepsilon\norm{\myvector{U}}}
\end{equation}
where $\myvector{e}_j$ is the unit vector with $j$th component equal to one, and zero in all other components. The value of $\varepsilon$ should be chosen small, as usual when calculating finite differences, but also large enough not to disturb the calculations due to problems caused by floating-point rounding calculations.  For all calculations in this work, the value $\varepsilon = \sqrt{\mathtt{eps}}$ was adequate, where $\mathtt{eps} = 2.22044604925031 \cdot 10^{-16}$ is the floating point accuracy for double precision.

The calculation of the forward finite difference approximation is a costly operation, especially for large systems, where it can be particularly prohibitive. However it offers a robust way to approximate the Jacobian. Strategies for improve the efficiency of this calculation exists, such as the use of analytic Jacobians, or use of the sparsity patterns of the Jacobian \parencite{kelleyIterativeMethodsLinear1995}. These are not treated in the present work. 
\subsubsection{Approximation of the Jacobian from differentiation of equation components}
The finite difference Jacobi matrix calculation shown above is a fairly simple but computationally expensive calculation. The \BoSSS code is capable of evaluating the Jacobian matrix automatically from the equation components given in Section \ref{ssec:SpatDiscretization}.
First, note that $\mathcal{A}(\myvector{U})$ could be written as
\begin{equation}
	[\mathcal{A}(\myvector{U})]_i = \mathcal{N}( \myvector{U}_h , \gvec{\Phi}_i) =
	\int_{\Omega_h}
	N_1 (\vec{x}, \myvector{U}_h , \nabla \myvector{U}_h  ) \cdot \gvec{\Phi}_i
	+ N_2 (\vec{x}, \myvector{U}_h , \nabla \myvector{U}_h  ) \cdot \nabla \gvec{\Phi}_i
	\textrm{dV}
	+
	\oint_{\Gamma} \ldots \mathrm{dS}.
	\label{eq:NonlinearGestalt}
\end{equation}
The edge integral, which is left out in \cref{eq:NonlinearGestalt},
can be expressed analogously to the volume integral, i.e. as a sum over
four nonlinear functions, multiplied by
$ \gvec{\Phi}_i^{+}$,  $\gvec{\Phi}_i^{-}$, $ \nabla \gvec{\Phi}_i^{+}$ and  $ \nabla \gvec{\Phi}_i^{-}$,
respectively.
These functions themselves may include the dependence on
$\vec{x}$, $\myvector{U}_h^{+}$,  $\myvector{U}_h^{-}$, $\nabla \myvector{U}_h^{+}$ and  $\nabla \myvector{U}_h^{-}$.
The is however omitted here for the sake of compactness, but the treatment is analogue. Realizing that 
\begin{equation}
\frac{\partial \myvector{U}_h}{\partial \myvector{U}_j}  = \gvec{\Phi}_j
\end{equation}
and by application of the chain rule, it is possible to derive an expression for the calculation of the entries of the Jacobian matrix from the equation components as
\begin{equation}
	\partial \mathcal{A}_{ij}(\myvector{U}) =
	\int_{\Omega_h}
	( \partial_{ \myvector{U}_h}       N_1 (\vec{x}, \myvector{U}_h , \nabla \myvector{U}_h  ) \gvec{\Phi}_j
	+   \partial_{\nabla \myvector{U}_h} N_1 (\vec{x}, \myvector{U}_h , \nabla \myvector{U}_h  ) \nabla \gvec{\Phi}_j ) \cdot \gvec{\Phi}_i
	+ \ldots
	\textrm{dV}
	+
	\oint_{\Gamma} \ldots \mathrm{dS} .
	\label{eq:JacobiGestalt}
\end{equation}
All omitted  terms in \cref{eq:JacobiGestalt} can be approximated analogously to the contributions for $N_1$. In the \BoSSS code, derivatives $ \partial_{ \myvector{U}_h} N_1 ( \ldots )$ and  $ \partial_{ \nabla \myvector{U}_h} N_1 ( \ldots )$ are approximated by a finite difference, using a perturbation by $\sqrt{\varepsilon}$ in the respective argument.

This approach has the significant improvement that it offers an efficient and accurate way to obtain the Jacobian matrix, unlike the two approaches mentioned above. For this reason, this option is the one chosen for the resolution of all the testcases shown in this work.

\subsection{Termination criterion}
\label{ssec:TerminationCriterion}
A simple approach to determine that a Newton-Dogleg loop can be terminated
is to check whether the residual norm has fallen below a certain threshold, i.e.
$ \| \mathcal{A}(U_n) \| \leq \textrm{tol}  $.
A universal choice for the tolerance is indeed difficult,
especially for investigations of convergence properties (cf.  \cref{ssec:ConvStudyHeatedCavity} and \cref{ss:UDF}).
If it is chosen too low, the algorithm may never terminate, because of dominating numerical round-off errors. On the other hand, if it is chosen too high, the error of the premature termination may dominate  the error of
the spatial discretization and one cannot take the full advantage of the high-order method.
Therefore the goal is to continue the Newton-Dogleg method until
the lowest possible limit dictated by floating point accuracy is reached.
To identify the limit in a robust way, we first define the residual-norm skyline as
\begin{equation}
	\textrm{sr}_n \coloneqq \min_{j \leq n} \| \mathcal{A}(\myvector{U}_j) \|
\end{equation}
and, for $n \geq 2$, the averaged reduction factor
\begin{equation}
	\textrm{arf}_n \coloneqq \frac{1}{2} \left(
	\frac{ \textrm{sr}_{n-2} }{  \max \{ \textrm{sr}_{n-1}, 10^{-100} \} }
	+  \frac{ \textrm{sr}_{n-1} }{  \max \{ \textrm{sr}_{n},   10^{-100} \} }
	\right) .
\end{equation}
The Newton-Dogleg method is terminated if
\begin{equation}
	n \geq 2 \text{ and }
	\textrm{sr}_n \leq 10^{-5} + 10^{-5} \| \myvector{U}_n \|_2 \text{ and }
	\textrm{arf}_n < 1.5 .
\end{equation}
For the computations in this work, this choice guarantees that the
nonlinear system is solved as accurately as possible. It secures that the numerical error is dominated by the error of the spatial or temporal discretization
and not by the termination criterion of the Newton-Dogleg method.
The skyline approach ensures robustness against oscillations close to the lower limit.


\section{Homotopy method}
\label{sec:HomotopyMethod}
Although the Newton-Dogleg method works well for a variety of cases,
we experienced convergence problems for some of the test cases presented in next section.
In particular, for the differentially heated cavity test case, the method was not successful on finding a convergent solution for a Rayleigh number $Ra \geq 10^5$  within 60 Newton iterations.
In such cases we used a homotopy strategy, which is loosely based on
ideas from the textbook of \textcite{deuflhardNewtonMethodsNonlinear2011}.
\tikzexternaldisable
\begin{figure}[t]
	\centering
	\pgfplotsset{
		compat=1.3,
		tick align = outside,
		yticklabel style={/pgf/number format/fixed},
	}
	\begin{tikzpicture}
		\begin{axis}[
			set layers=standard,
			width=0.75\linewidth,
			height=6cm,
			axis y line*=left,
			ymode = log,
			xlabel=Iteration number,
			xmin = 0,
			ylabel=Residual and trust region $\delta$,
			xtick = {0,2,...,36},
			ytickten = {-16,-12,...,12}
			]
			\addplot[ blue, mark =square*, mark size = 2pt] file{data/ConvergenceStory_HeatedCavity_withHomotopy/delta3.txt};\label{plot_one}
			\addplot[ orange, mark =o, mark size = 2.5pt] file{data/ConvergenceStory_HeatedCavity_withHomotopy/residuals3.txt};\label{plot_two}
		\end{axis}
		\begin{axis}[
			axis y line=right,
			width=0.75\linewidth,
			height=6cm,
			axis x line=none,
			ylabel= Homotopy parameter hp,
			xmin = 0,
			ymin = 50,
			ymax = 1100,
			legend style={at={(0.1,0.99)},anchor=north west,},
			xtick={0,5,...,35},
			ytick ={100,200,...,1000},
			]
			\addlegendimage{/pgfplots/refstyle=plot_two}\addlegendentry{$	\| \mathcal{A}_{\mathrm{hp}^*}(\myvector{U}_{n}) \|_2 $}
			\addlegendimage{/pgfplots/refstyle=plot_one}\addlegendentry{$\delta$}
			\addplot[red , mark =x, mark size = 2.5pt] file{data/ConvergenceStory_HeatedCavity_withHomotopy/reynolds3.txt};
			\addlegendentry{$\mathrm{hp}$}
		\end{axis}
	\end{tikzpicture}
	\caption{behavior of the homotopy method for the differentially heated cavity test case. The homotopy parameter $\mathrm{hp}$ in this case is the Reynolds number. }
	\label{fig:Homotopyevolution}
\end{figure}

\tikzexternalenable
We start by identifying a parameter that makes the solution of the nonlinear problem difficult to solve. In the following we will refer to this variable as the homotopy parameter.
The main idea of the homotopy strategy consists of solving a series of simpler problems, starting with a parameter where the problem is easy to solve, and carefully increasing it until the desired value is reached.  Let $\mathrm{Hp}$ denote the value of the homotopy parameter for which a solution is being sought. Let
\begin{equation}
	\mathcal{A}_{\mathrm{hp}^*}(\myvector{U}) = 0
	\label{eq:NonlinearAt-hp}
\end{equation}
be the discretized system for a certain intermediate homotopy-parameter
$\mathrm{hp}^*$, between 0 and the `target'  homotopy-parameter $\mathrm{Hp}$, i.e. $0 \leq \mathrm{hp}^* \leq \mathrm{Hp}$.
Furthermore, let $\myvector{U}_{\mathrm{hp},\epsilon} $ be an approximate solution
to the problem (\ref{eq:NonlinearAt-hp}) with $ \mathrm{hp}^* =  \mathrm{hp}$,
up to a tolerance $\epsilon$, i.e.
\begin{equation}
	\left\| \mathcal{A}_{\mathrm{hp}}( \myvector{U}_{\mathrm{hp},\epsilon} ) \right\|_2 \leq \epsilon .
\end{equation}
For the sake of clarity when discussing the algorithm which follows below, we distinguish between the
intermediate homotopy-parameter  $\mathrm{hp}$ for which we assume to already have found an acceptable solution
and the next homotopy-parameter $\mathrm{hp}^*$ that we are currently trying to find a solution for.
For any $\textrm{hp}^* > \textrm{hp}$
we set
$
\epsilon
= 10^{-5}
\left\| \mathcal{A}_{\mathrm{hp*}}( \myvector{U}_{\mathrm{hp},\epsilon} ) \right\|_2
$,
i.e. we aim for a residual norm reduction of at least five orders of magnitude with respect to the initial residual norm.
If  $\textrm{hp}^* = \textrm{Hp}$, the termination criterion presented in section \ref{ssec:TerminationCriterion} is applied.
An approximate solution for the target homotopy-parameter is found by the following recipe:
\begin{itemize}
	\item[(1)]
	Set $\mathrm{hp} = 0$, i.e. start by obtaining an (approximate) solution $\myvector{U}_{0,\epsilon}$.
	
	\item[(2)]
	Search for a an increased homotopy-parameter $\mathrm{hp}^*$:
	Find the minimal $i \geq 0$ so that for
	$
	\mathrm{hp}^* = \frac{1}{2^i}(\mathrm{Hp} - \mathrm{hp}) + \mathrm{hp}
	$
	one has
	$
	\left\| \mathcal{A}_{\mathrm{hp}^*}(\myvector{U}_{\mathrm{hp},\epsilon}) \right\|_2
	\leq
	\delta_{\textrm{max}} \left\| \mathcal{A}_{\mathrm{hp}}(\myvector{U}_{\mathrm{hp},\epsilon}) \right\|_2
	$
	Here, $\delta_{\textrm{max}}$ is the maximal allowed increase of the residual for an increased
	homotopy-parameter $\mathrm{hp}^*$;  $\delta_{\textrm{max}}$ is adapted in the following steps,
	as an initial guess we use $ \delta_{\textrm{max}} = 10^6$.
	
	\item[(3)]
	Use the Newton-Dogleg method to compute an approximate solution to the problem (\ref{eq:NonlinearAt-hp}),
	for the homotopy-parameter $\mathrm{hp}^*$,
	using the solution $\myvector{U}_{\mathrm{hp},\epsilon}$ as an initial guess.
	
	\begin{itemize}
		\item
		If the Newton-Dogleg method did not converge successfully within ten steps,
		the homotopy-parameter increase from $\mathrm{hp}$ to $\mathrm{hp}^*$ was probably too large.
		Set $\delta_{\textrm{max}} \leftarrow 0.2\delta_{\textrm{max}}$ and go to step (2).
		
		\item
		If the Newton-Dogleg method reached its convergence criterion and
		if the target homotopy-parameter is reached, i.e. $\mathrm{hp}^* = \mathrm{Hp}$,
		the algorithm has successfully found an approximate solution
		for $ \mathcal{A}_{\mathrm{Hp}}(\myvector{U}) = 0$ and can terminate.
		
		\item
		Otherwise, if the Newton-Dogleg method converged successfully, but is below the target homotopy-parameter:
		Accept the solution and set $\mathrm{hp} \leftarrow \mathrm{hp}^*$.
		If the Newton-Dogleg method took less than three iterations to reach the convergence criterion,
		set  $\delta_{\textrm{max}} \leftarrow 8\delta_{\textrm{max}}$.
		Return to step (2).
	\end{itemize}
	
\end{itemize}
An exemplary run of the method is shown in \cref{fig:Homotopyevolution}. The homotopy parameter $\mathrm{hp}$ in this particular case is the Reynolds number. The homotopy-parameter hp was increased for iterations 10, 18, 22 and 24, causing an increase of the residuals $\| \mathcal{A}_{\mathrm{hp}^*}(\myvector{U}_{n}) \|_2 $, leading to a convergent solution after 34 Newton iterations. The presented algorithm offers a robust method for finding steady-state solutions of highly nonlinear systems.

\section{Initialization of combustion applications}\label{ssec:MethodCombustion}
The proposed algorithm for obtaining steady state solutions of finite reaction rate combustion problems involves first solving the problem assuming an infinite reaction rate (the flame sheet problem). This requires first solving the system presented in \cref{sec:FlameSheet}, where \cref{DiscretizedConti2,DiscretizedMomentum2,DiscretizedEnergy2} need to be solved in a coupled manner together with the expressions that link the temperature and mass fractions to the mixture fraction in order to be able to evaluate the density and . This idea has been already employed in various works \parencite{smookeNumericalSolutionTwoDimensional1986,smookeNumericalModelingAxisymmetric1992}. The reason for the use of this pre-step is twofold:
\begin{itemize}
	\item The system of  \cref*{eq:LowMach_Conti,eq:LowMach_Momentum,eq:LowMachEnergy,eq:LowMachMassBalance} presents multiple solutions. One is the pure mixing (frozen) solution, where no chemical reaction has taken place, and the other one is the ignited solution, where the flame is present. Using the flame sheet solution as initial estimate ensures that the path taken by Newton's algorithm will tend towards the ignited solution.
		\item Solving \cref*{eq:LowMach_Conti,eq:LowMach_Momentum,eq:LowMachEnergy,eq:LowMachMassBalance} using a Newton-type method requires adequate starting estimates in order to converge. Using the flame sheet solution as initial estimate improves the convergence properties of the method.
\end{itemize}

It should be noted regarding the solution of the flame sheet problem (cf. \cref{sec:FlameSheet}) that the sharp change in the primitive variables around $z = z_{st}$  is problematic in certain scenarios. In particular, the non-smoothness of the derived variables could lead to Gibbs phenomenon-type problems. This inconvenient can be remedied by using a regularized form of the equations. The smoothing function $\mathcal{H}$ is defined as
\begin{equation}\label{eq:regularization_MF}
	\mathcal{H}(z) \approx \frac{1}{2}(1+\tanh(\sigma(z - z_{st}) )),
\end{equation}
This function is useful for creating a smooth transition between two functions, since it returns values close to 0 for $z \ll z_{st}$ and values close to 1 for $z \gg z_{st}$. The sharpness of the transition at the point $z = z_{st}$ is dictated by the parameter $\sigma$. In \cref{fig:SmoothingFunc} the smoothing function $H$ using different smoothing parameters $\sigma$ is shown. Clearly, increasing the value of $\sigma$ increases the sharpness of the transition at the point $z_{st}$. For a very big $\sigma$ value the function $H$ resembles the Heaviside step function
\begin{figure}[h]
	\centering
	\inputtikz{SmoothingFunc}
	\caption{Smoothing function  at $z_{\text{st}} = 0.22$ for different smoothing parameters $\sigma$. }\label{fig:SmoothingFunc}
\end{figure}
 Using \cref{eq:regularization_MF} the temperature and mass fraction fields can be written as
\begin{subequations}
	\begin{align}
		T(z)   & = z T_F^0 + (1-z)T_O^0 + \frac{Q Y_F^0}{c_p} z_{st}\frac{1- z}{1-z_{st}}\mathcal{H}(z) +  \frac{Q Y_F^0}{c_p}z\left(1-\mathcal{H}(z)\right),  \label{eq:BS-TR} \\[1ex]
		Y_F(z) & = Y_F^0\frac{z - z_{st}}{1-z_{st}} \mathcal{H}(z), \label{eq:BS-YFR}                                                                                           \\[1ex]
		Y_O(z) & = Y_O^0 \frac{z_{st}-z}{z_{st}} (1-\mathcal{H}(z)), \label{eq:BS-YOR}                                                                                          \\[1ex]
		Y_P(z) & =  Y_O^0\frac{M_P\nu_P}{M_O\nu_O}(1-z)\mathcal{H}(z) +	Y_F^0\frac{M_P\nu_P}{M_F\nu_F}z (1-\mathcal{H}(z)), \label{eq:BS-YPR}                                   \\[1ex]
		Y_N(z) & = (1-Y_F^0)z + (1-Y_O^0)(1-z). \label{eq:BS-YNR}
	\end{align}
\end{subequations}
The use of this regularized form of the equations results in practice on a spreading of the flame front, which eases the numerical calculation \parencite{braackAdaptiveFiniteElement1997}.
\begin{figure}[h]
	\centering
	\inputtikz{SmoothingPicture}
	\caption{Temperature profile calculated in the center-line of a counter-flow flame configuration for different smoothing parameters $\sigma$.}
	\label{fig:smoothings}
\end{figure}
In \cref{fig:smoothings} the effect of the smoothing factor $\sigma$ on calculations of a flame in a counter-flow configuration are shown. It can be clearly observed how for decreasing $\sigma$ the solution becomes smoother.

One question one could certainly ask is under what flame conditions the infinite reaction rate solution (also called flame sheet solution in the following) effectively is a good initial estimate for Newton's algorithm.  Obviously for systems that respect the assumptions done for the flame sheet the obtained solution will be very close to the finite-rate solution (see \cref{fig:MixtureFraction_finiteRateComparison}). The assumption of an infinitely fast chemical reaction implies that the time scales associated with the chemical reaction are infinitely smaller than the flow scales, or in other words, $\text{Da} \to \infty$. For this reason, the flame sheet solution is expected to give a similar solution for cases close to equilibrium (where the Damk√∂hler number is large). On the other hand, in cases that are far from equilibrium, as, for example, in the case of a flame in conditions close to extinction, it is expected that the flame sheet solution will depart considerably from the solution with a finite reaction rate.

It should be noted that within the derivation of the equations for the flame sheet it is only assumed that the heat capacity is the same for all components ($c_{kp} = c_p$), but it is still possible to consider a dependence on temperature. However, this introduces a difficulty, since the evaluation of the temperature with \cref{eq:BS-YF} requires $c_p$, which according to \cref{eq:nondim_cpmixture}, depends in turn on the temperature. Solving the system of equations required to obtain $c_p$ and $T$ is very expensive, since it would require solving it every time the temperature must be evaluated -in particular for the evaluation of the density $\rho$ and transport parameters $\mu$ and $\rho D$-.  This problem can be solved by simply assuming a constant representative value of $c_p$. 

The problem that now arises is the selection of a suitable $c_p$. In the work by \textcite{xuApplicationPrimitiveVariable1993} it is suggested to estimate it simply on the basis of experimental measurements, or also by selecting some representative value, such as $c_p$ evaluated at the adiabatic temperature and stoichiometric conditions. In particular, in this work the value $\hat c_p = \SI{1.3}{\kilo \joule \per \kilo \gram \per \kelvin}$ was adequate for all calculations. This constant value of the heat capacity proved to yield a flame sheet solution which is an adequate estimate for finite-rate simulations, even for cases with a nonconstant heat capacity. 

In a similar fashion, the assumption of unity Lewis number in the flame sheet system delivers a solution that slightly deviates from the solution of the finite chemistry rate problem with nonunity Lewis numbers. Nevertheless, this small deviation does not preclude the use of the flame sheet solution as an adequate initial estimate for Newton's method. 


\subsubsection{Adaptive Mesh Refinement}\label{ssec:MeshRefinement}
The area where the chemical reaction takes place is usually a thin region whose thickness is defined by the availability of reactants.
\begin{figure}
	\centering
	\pgfplotsset{width=0.50\textwidth, compat=1.3}
	\inputtikz{MeshRefinementCoflow1}
	\inputtikz{MeshRefinementCoflow2}
	\inputtikz{MeshRefinementCoflow3}
	\caption{Adaptive mesh refinement around the stoichiometric surface in a coflow flame configuration.}
\end{figure}
\subsection{Solver safeguard}
Another strategy that proved to be useful in improving the convergence properties of the iterative scheme is the use of a solver safeguard, which is used to avoid unphysical solutions during the solution procedure, such as negative temperatures or temperatures higher than the adiabatic temperature. In particular, the idea is to clip values from the solution fields delivered by Newton's algorithm which are known to be unphysical, and limit the solution fields by user defined values. This clipping emulates in a sense the effect of schemes such as \gls{TVM} or \gls{ENO} \parencite{nicoudConservativeHighOrderFiniteDifference2000}. 

Particularly, for an arbitrary scalar $\xi$ the values are bounded in the range $[\xi_{\text{min}} - \epsilon_{\text{safe}}, \xi_{\text{max}} + \epsilon_{\text{safe}}]$, where $\xi_{\text{min}}$ and $\xi_{\text{max}}$ are user defined bounds, and  $\epsilon_{\text{safe}} = 10^{-4}$. For example, the mass fractions by definition should have a value between zero and one, thus $Y_{k,\text{min}} = 0$ and  $Y_{k,\text{max}} = 1$. For certain problems, particularly problems involving combustion, it could be also useful to limit the value of the temperature, which can be bounded using the inlet conditions as the minimum value, and the adiabatic temperature as the maximum value.

The occurrence of these non-physical values is not always problematic, and in theory the Newton algorithm above should be able to correctly handle them and finally find the solution of the nonlinear system. However in certain cases this can lead to problems. Just to mention one example, a negative temperature would result in a imaginary value of the viscosity if it is calculated according to \cref{eq:nondim_sutherland}. Particularly for problems with sharp gradients this could be problematic due to dispersion phenomena. 